{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models & Chat Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 23, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a193010-e16f-4e9f-8426-21717e62590a-0', usage_metadata={'input_tokens': 23, 'output_tokens': 3, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English to Simplified Chinese.\"),\n",
    "    HumanMessage(\"Hi!\")\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Growing a tomato from seed to harvest typically takes about 70 to 100 days, depending on the variety of tomato and the growing conditions. Here's a general timeline:\\n\\n1. **Seed Germination**: Tomato seeds usually take about 5 to 14 days to germinate after planting in ideal conditions (warm soil, moisture).\\n\\n2. **Seedling Stage**: Once germinated, the seedlings will grow for about 4 to 6 weeks (30 to 42 days) before they are ready to be transplanted outdoors or into larger pots.\\n\\n3. **Transplanting**: If you transplant seedlings after the last frost, they will need additional time to establish themselves in their new environment.\\n\\n4. **Fruit Development**: After transplanting, it generally takes another 50 to 80 days for the plants to mature and produce ripe fruit, depending on the variety.\\n\\nOverall, from planting the seed to harvesting ripe tomatoes, you can expect the process to take about 3 to 4 months. Factors such as temperature, sunlight, watering, and soil quality can all influence the growth rate.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 223, 'prompt_tokens': 22, 'total_tokens': 245, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-3d175e92-ecde-41f5-befa-e5311549d60d-0', usage_metadata={'input_tokens': 22, 'output_tokens': 223, 'total_tokens': 245, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('How long would it take to grow a tomato from one of its seed?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--你好--！----"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Will you be my forever friend?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 25, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-4294a36c-3a3f-4a17-a88e-5706a35d2802-0', usage_metadata={'input_tokens': 25, 'output_tokens': 8, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([\n",
    "    {'role': 'system', 'content': \"convert the sentence into a child's tone in English\"},\n",
    "    {'role': 'user', 'content': 'Will you marry me?'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template =  \"Translate the following from English to {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English to Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"Hello!\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English to Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 20, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-5ec04c3c-3e5f-4614-8986-316430752eb7-0', usage_metadata={'input_tokens': 20, 'output_tokens': 4, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search over PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are greate human friends, known for their loyalies and cuteness.\",\n",
    "        metadata={'source': \"my creativity\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are evil animals that steal human hearts.\",\n",
    "        metadata={'source': 'my brain'}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf'\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0}, page_content='COVER LETTER\\nDear Hiring Manager at TrustingSocial,\\nI am writing to express my keen interest in the Machine Learning Engineer  position at TrustingSocial, \\nas advertised on LinkedIn. With a Bachelor Degree in AI and a year of experience as an AI Engineer, \\nhands-on with projects that go deep into recommendation systems, computer vision, and generative AI, I \\nbelieve that the skills I bring are a good fit for the team.\\nDuring my tenure as an AI Engineer, I actively participated in various projects, one of them was \\nRecommendation Systems. Those projects provided me with valuable challenges and opportunities to \\nbuild and train models that give meaningful suggestions to users to keep them engaged in our products. In \\na side projects where I learn things as a hobby, I applied Machine Learning knowledges collected from \\ncollege into a real-life medical data project where I researched and picked suitable Machine Learning \\nalgorithms for regressing and classifying tasks, I also built a neuron network model to predict the data to \\ncompare between machine learning and deep learning solutions on the same problem.  \\nAdditionally, I also had a short-time joining a Data Engineer project where we build data transformation \\npipelines on Databricks, this experience has sharpened my SQL querying skill as well as equipping me \\nwith insight of how large data can be stored by Delta tables and processed by Spark operations.\\nI am particularly fascinated by TrustingSocial as their mission of bridging the gap between banks and \\npotential customers among people that not yet have bank-record that need loans. I believe that this vision \\nis yet and will push the international economy developments, especially in developing countries like \\nVietnam, Philippines, and India.\\nI have attached my resume for your review, which provides more details about my qualifications and \\nprojects. I would appreciate the opportunity to discuss my suitability for this position further and learn \\nmore about the specific challenges and opportunities within your team.\\nThank you for your time and consideration.\\nSincerely,\\nKhoi Nguyen The\\nMobile: 0901954436\\nEmail: nguyenthekhoig7@gmail.com\\nLinkedin: https://www.linkedin.com/in/nguyenthekhoig7/')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COVER LETTER\\nDear Hiring Manager at TrustingSocial,\\nI am writing to express my keen interest in the Machine Learning Engineer  position at TrustingSocial, \\nas advertised on LinkedIn. With a Bachelor Degree in AI and a year of experience as an AI Engineer, \\nhands-on with projects that go deep into recommendation systems, computer vision, and generative AI, I \\nbelieve that the skills I bring are a good fit for the team.\\nDuring my tenure as an AI Engineer, I actively participated in various projects, one of them was \\nRecommendation Systems. Those projects provided me with valuable challenges and opportunities to \\nbuild and train models that give meaningful suggestions to users to keep them engaged in our products. In \\na side projects where I learn things as a hobby, I applied Machine Learning knowledges collected from \\ncollege into a real-life medical data project where I researched and picked suitable Machine Learning \\nalgorithms for regressing and classifying tasks, I also built a neuron network model to predict the data to \\ncompare between machine learning and deep learning solutions on the same problem.  \\nAdditionally, I also had a short-time joining a Data Engineer project where we build data transformation \\npipelines on Databricks, this experience has sharpened my SQL querying skill as well as equipping me \\nwith insight of how large data can be stored by Delta tables and processed by Spark operations.\\nI am particularly fascinated by TrustingSocial as their mission of bridging the gap between banks and \\npotential customers among people that not yet have bank-record that need loans. I believe that this vision \\nis yet and will push the international economy developments, especially in developing countries like \\nVietnam, Philippines, and India.\\nI have attached my resume for your review, which provides more details about my qualifications and \\nprojects. I would appreciate the opportunity to discuss my suitability for this position further and learn \\nmore about the specific challenges and opportunities within your team.\\nThank you for your time and consideration.\\nSincerely,\\nKhoi Nguyen The\\nMobile: 0901954436\\nEmail: nguyenthekhoig7@gmail.com\\nLinkedin: https://www.linkedin.com/in/nguyenthekhoig7/'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100, add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of vector 1: 3072\n",
      "content of vector 1: [0.00021363057021517307, 0.017445873469114304, -0.005530727095901966, -0.05096905678510666, -0.006191757041960955, -0.004404651001095772, -0.028168508782982826, 0.029523786157369614, -0.0206480473279953, 0.0015861394349485636, 0.019624946638941765, -0.015373095870018005, 0.007161710411310196, -0.0018120190361514688, 0.0046670702286064625, 0.018336104229092598, 0.00662358570843935, 0.018282955512404442, -0.02242851071059704, -0.0431828536093235, 0.02840767428278923, -0.015014346688985825, -0.009991847909986973, 0.033749062567949295, 0.010430320166051388, -0.013200666755437851, -0.011685945093631744, -0.0037502648774534464, -0.012861846946179867, -0.011739092878997326, 0.04243877902626991, -0.008071872405707836, 0.04437868669629097, -0.006982335355132818, 0.007978863082826138, -0.03789461404085159, 0.03765545040369034, 0.045733965933322906, -0.012396801263093948, 0.028646841645240784, 0.02048860304057598, 0.02856711857020855, -0.02458100952208042, -0.011127889156341553, -0.011725805699825287, -0.0007810283568687737, 0.020315872505307198, -0.022707538679242134, -0.023199157789349556, -0.0011842066887766123, 0.00908832997083664, -0.002674015238881111, 0.023743925616145134, 0.024527860805392265, -0.06462812423706055, -0.0006776386289857328, 0.024926472455263138, -0.050756461918354034, -0.0035874987952411175, -0.0004048392584081739, -0.008762797340750694, 0.03380221128463745, -0.021418696269392967, -0.023398464545607567, -0.010084857232868671, -0.0335630439221859, 0.06058886647224426, 0.0019050282426178455, -0.02152499184012413, 0.028381099924445152, -0.0364861898124218, -0.029656656086444855, 0.026839805766940117, 0.008935528807342052, 0.031995173543691635, 0.03428054228425026, -0.01808365061879158, 0.04655776172876358, 0.007573608309030533, -0.022136196494102478, 0.02350476011633873, -0.04235905781388283, -0.0026158844120800495, 0.02885943464934826, 0.03451970964670181, 0.03279239684343338, -0.006298053078353405, -0.09609181433916092, -0.048524241894483566, 0.004461121279746294, -0.04347516968846321, 0.009513515047729015, 0.023650918155908585, -0.00620836578309536, 0.010410389862954617, 0.0017954101786017418, 0.03133082017302513, 0.001046353834681213, -0.008291108533740044, 0.01934591867029667]\n"
     ]
    }
   ],
   "source": [
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2), \"why the same model produce different len?\"\n",
    "\n",
    "print(f\"len of vector 1: {len(vector_1)}\")\n",
    "print(f\"content of vector 1: {vector_1[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "ids = vector_store.add_documents(documents = all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"What are the reasons that I am particularly fascinated by TrustingSocial?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8c0bb049-bb4b-420c-9c2c-679a4241a37d', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 931}, page_content='algorithms for regressing and classifying tasks, I also built a neuron network model to predict the data to \\ncompare between machine learning and deep learning solutions on the same problem.  \\nAdditionally, I also had a short-time joining a Data Engineer project where we build data transformation \\npipelines on Databricks, this experience has sharpened my SQL querying skill as well as equipping me \\nwith insight of how large data can be stored by Delta tables and processed by Spark operations.\\nI am particularly fascinated by TrustingSocial as their mission of bridging the gap between banks and \\npotential customers among people that not yet have bank-record that need loans. I believe that this vision \\nis yet and will push the international economy developments, especially in developing countries like \\nVietnam, Philippines, and India.\\nI have attached my resume for your review, which provides more details about my qualifications and'),\n",
       " Document(id='d811a22e-638e-453f-a345-b043e98fa674', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 0}, page_content='COVER LETTER\\nDear Hiring Manager at TrustingSocial,\\nI am writing to express my keen interest in the Machine Learning Engineer  position at TrustingSocial, \\nas advertised on LinkedIn. With a Bachelor Degree in AI and a year of experience as an AI Engineer, \\nhands-on with projects that go deep into recommendation systems, computer vision, and generative AI, I \\nbelieve that the skills I bring are a good fit for the team.\\nDuring my tenure as an AI Engineer, I actively participated in various projects, one of them was \\nRecommendation Systems. Those projects provided me with valuable challenges and opportunities to \\nbuild and train models that give meaningful suggestions to users to keep them engaged in our products. In \\na side projects where I learn things as a hobby, I applied Machine Learning knowledges collected from \\ncollege into a real-life medical data project where I researched and picked suitable Machine Learning'),\n",
       " Document(id='fd960435-9fa0-463f-84b6-c58ed505570e', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 1774}, page_content='I have attached my resume for your review, which provides more details about my qualifications and \\nprojects. I would appreciate the opportunity to discuss my suitability for this position further and learn \\nmore about the specific challenges and opportunities within your team.\\nThank you for your time and consideration.\\nSincerely,\\nKhoi Nguyen The\\nMobile: 0901954436\\nEmail: nguyenthekhoig7@gmail.com\\nLinkedin: https://www.linkedin.com/in/nguyenthekhoig7/')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'algorithms for regressing and classifying tasks, I also built a neuron network model to predict the data to \\ncompare between machine learning and deep learning solutions on the same problem.  \\nAdditionally, I also had a short-time joining a Data Engineer project where we build data transformation \\npipelines on Databricks, this experience has sharpened my SQL querying skill as well as equipping me \\nwith insight of how large data can be stored by Delta tables and processed by Spark operations.\\nI am particularly fascinated by TrustingSocial as their mission of bridging the gap between banks and \\npotential customers among people that not yet have bank-record that need loans. I believe that this vision \\nis yet and will push the international economy developments, especially in developing countries like \\nVietnam, Philippines, and India.\\nI have attached my resume for your review, which provides more details about my qualifications and'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_asyn = await vector_store.asimilarity_search(\n",
    "    \"What are projects that I have partitcipated in?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8c0bb049-bb4b-420c-9c2c-679a4241a37d', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 931}, page_content='algorithms for regressing and classifying tasks, I also built a neuron network model to predict the data to \\ncompare between machine learning and deep learning solutions on the same problem.  \\nAdditionally, I also had a short-time joining a Data Engineer project where we build data transformation \\npipelines on Databricks, this experience has sharpened my SQL querying skill as well as equipping me \\nwith insight of how large data can be stored by Delta tables and processed by Spark operations.\\nI am particularly fascinated by TrustingSocial as their mission of bridging the gap between banks and \\npotential customers among people that not yet have bank-record that need loans. I believe that this vision \\nis yet and will push the international economy developments, especially in developing countries like \\nVietnam, Philippines, and India.\\nI have attached my resume for your review, which provides more details about my qualifications and'),\n",
       " Document(id='fd960435-9fa0-463f-84b6-c58ed505570e', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 1774}, page_content='I have attached my resume for your review, which provides more details about my qualifications and \\nprojects. I would appreciate the opportunity to discuss my suitability for this position further and learn \\nmore about the specific challenges and opportunities within your team.\\nThank you for your time and consideration.\\nSincerely,\\nKhoi Nguyen The\\nMobile: 0901954436\\nEmail: nguyenthekhoig7@gmail.com\\nLinkedin: https://www.linkedin.com/in/nguyenthekhoig7/'),\n",
       " Document(id='d811a22e-638e-453f-a345-b043e98fa674', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 0}, page_content='COVER LETTER\\nDear Hiring Manager at TrustingSocial,\\nI am writing to express my keen interest in the Machine Learning Engineer  position at TrustingSocial, \\nas advertised on LinkedIn. With a Bachelor Degree in AI and a year of experience as an AI Engineer, \\nhands-on with projects that go deep into recommendation systems, computer vision, and generative AI, I \\nbelieve that the skills I bring are a good fit for the team.\\nDuring my tenure as an AI Engineer, I actively participated in various projects, one of them was \\nRecommendation Systems. Those projects provided me with valuable challenges and opportunities to \\nbuild and train models that give meaningful suggestions to users to keep them engaged in our products. In \\na side projects where I learn things as a hobby, I applied Machine Learning knowledges collected from \\ncollege into a real-life medical data project where I researched and picked suitable Machine Learning')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_asyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='8c0bb049-bb4b-420c-9c2c-679a4241a37d', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 931}, page_content='algorithms for regressing and classifying tasks, I also built a neuron network model to predict the data to \\ncompare between machine learning and deep learning solutions on the same problem.  \\nAdditionally, I also had a short-time joining a Data Engineer project where we build data transformation \\npipelines on Databricks, this experience has sharpened my SQL querying skill as well as equipping me \\nwith insight of how large data can be stored by Delta tables and processed by Spark operations.\\nI am particularly fascinated by TrustingSocial as their mission of bridging the gap between banks and \\npotential customers among people that not yet have bank-record that need loans. I believe that this vision \\nis yet and will push the international economy developments, especially in developing countries like \\nVietnam, Philippines, and India.\\nI have attached my resume for your review, which provides more details about my qualifications and')],\n",
       " [Document(id='fd960435-9fa0-463f-84b6-c58ed505570e', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 1774}, page_content='I have attached my resume for your review, which provides more details about my qualifications and \\nprojects. I would appreciate the opportunity to discuss my suitability for this position further and learn \\nmore about the specific challenges and opportunities within your team.\\nThank you for your time and consideration.\\nSincerely,\\nKhoi Nguyen The\\nMobile: 0901954436\\nEmail: nguyenthekhoig7@gmail.com\\nLinkedin: https://www.linkedin.com/in/nguyenthekhoig7/')]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    return vector_store.similarity_search(query, k = 1)\n",
    "\n",
    "\n",
    "retriever.batch([\n",
    "    \"How many projects have I participated in?\",\n",
    "    \"Did I attach a resume with the document?\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8c0bb049-bb4b-420c-9c2c-679a4241a37d', metadata={'source': '/home/thekhoi/Downloads/TheKhoi_AIEngineer_Application/TheKhoi_CoverLetter_AIEngineer.pdf', 'page': 0, 'start_index': 931}, page_content='algorithms for regressing and classifying tasks, I also built a neuron network model to predict the data to \\ncompare between machine learning and deep learning solutions on the same problem.  \\nAdditionally, I also had a short-time joining a Data Engineer project where we build data transformation \\npipelines on Databricks, this experience has sharpened my SQL querying skill as well as equipping me \\nwith insight of how large data can be stored by Delta tables and processed by Spark operations.\\nI am particularly fascinated by TrustingSocial as their mission of bridging the gap between banks and \\npotential customers among people that not yet have bank-record that need loans. I believe that this vision \\nis yet and will push the international economy developments, especially in developing countries like \\nVietnam, Philippines, and India.\\nI have attached my resume for your review, which provides more details about my qualifications and')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever2 = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": 1}\n",
    ")\n",
    "\n",
    "retriever2.invoke(\n",
    "    \"How many degrees do I hold?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"\"Information about a person\"\"\"\n",
    "\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    height_in_meters: Optional[float] = Field(default=None, description=\"Height of the person, in meters\")\n",
    "    hair_color: Optional[str] = Field(default = None, description=\"Color of the person's hair\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"You are an expert at extracting information from text\"\n",
    "         \"If you do not know the value of any attribute\"\n",
    "         \"Just return None\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Donald Trump has long blonde hair and is 7 feet tall.\"\n",
    "\n",
    "prompt = prompt_template.invoke({\"text\", text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Donald Trump', height_in_meters=2.13, hair_color='blonde')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class Data(BaseModel):\n",
    "    people: Optional[List[Person]] #= Field(default=None, description=\"List of multiple people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm2 = llm.with_structured_output(schema=Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='John', height_in_meters=1.9812, hair_color=None), Person(name='Andy', height_in_meters=1.8288, hair_color='black curly')])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm2.invoke(prompt_template.invoke({\"text\": \"John is 6.5 feet tall. Andy is 6 feet tall with black curly hair.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tool_example_to_messages to give fewshot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import tool_example_to_messages\n",
    "\n",
    "examples = [\n",
    "    (\"The ocean is vast and blue, it is over 20,000 feet deep.\",\n",
    "     Data(people = [])),\n",
    "     (\"Kelly travelled from France to Japan.\",\n",
    "      Data(people = [Person(name='Kelly', height_in_meters=None, hair_color=None)]))\n",
    "]\n",
    "\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3886255/2704284911.py:7: LangChainBetaWarning: The function `tool_example_to_messages` is in beta. It is actively being worked on, so the API may change.\n",
      "  messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n"
     ]
    }
   ],
   "source": [
    "for txt, tool_call in examples:\n",
    "    if tool_call.people:\n",
    "        ai_response = 'Detected people'\n",
    "    else:\n",
    "        ai_response = 'No people detected'\n",
    "\n",
    "    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The ocean is vast and blue, it is over 20,000 feet deep.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Data (51c9b586-5440-49e9-be25-aa1ef430b816)\n",
      " Call ID: 51c9b586-5440-49e9-be25-aa1ef430b816\n",
      "  Args:\n",
      "    people: []\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You have correctly called this tool.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "No people detected\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Kelly travelled from France to Japan.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Data (5d5851b7-8f09-4a02-9a41-82882083fc4a)\n",
      " Call ID: 5d5851b7-8f09-4a02-9a41-82882083fc4a\n",
      "  Args:\n",
      "    people: [{'name': 'Kelly', 'height_in_meters': None, 'hair_color': None}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You have correctly called this tool.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Detected people\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Earth', height_in_meters=0.0, hair_color='none')])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_no_extraction = {\n",
    "    'role': 'user',\n",
    "    'content': 'The solar system is large, but earth has only 1 moon.'\n",
    "}\n",
    "\n",
    "structured_llm2 = llm.with_structured_output(schema=Data)\n",
    "structured_llm2.invoke([messages_no_extraction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm2.invoke(messages + [messages_no_extraction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Khoi! Nice to meet you too! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 17, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-308f7006-b12f-4925-bdee-cba1836e40f2-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"Hi I am Khoi, nice to meet you\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 13, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-bdd3f432-4664-464a-a784-51a8df4921d0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 28, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content='What is my namme?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Khoi. Nice to meet you!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 41, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-46e7c37e-061d-494f-b131-6827ba5d7a33-0', usage_metadata={'input_tokens': 41, 'output_tokens': 11, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke([\n",
    "    HumanMessage('Hi I am Khoi, nice to meet you'),\n",
    "    AIMessage('Hi Khoi, how can I assist you today?'),\n",
    "    HumanMessage('What is my name?')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph \n",
    "\n",
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state['messages'])\n",
    "    return {'messages': response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, 'model')\n",
    "workflow.add_node('model', call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable': {\"thread_id\": 'abcabc123'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Hi, I am Khoi.'\n",
    "\n",
    "input_message = HumanMessage(query)\n",
    "output = app.invoke({'messages': [input_message]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, I am Khoi.', additional_kwargs={}, response_metadata={}, id='75ccc6d3-892e-4fb7-8d06-ca6816394879'),\n",
       " AIMessage(content='Hi Khoi! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 14, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-79b8105a-9cbc-42d5-b5c3-544273d89745-0', usage_metadata={'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Khoi. How can I help you today, Khoi?\n"
     ]
    }
   ],
   "source": [
    "query = 'What is my name?'\n",
    "input_message = HumanMessage(query)\n",
    "\n",
    "output = app.invoke({'messages': [input_message]}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. Therefore, I don't know your name. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config2 = {'configurable': {'thread_id': 'abc123456'}}\n",
    "input_message = 'What is my name?'\n",
    "output = app.invoke({'messages': [input_message]}, config2)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Khoi. If you have any other questions or need assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "input_message = 'What is my name?'\n",
    "output = app.invoke({'messages': [input_message]}, config)\n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate, answer all questions to the best of your character.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {'messages': response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, 'model')\n",
    "workflow.add_node('model', call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, Tim! What brings ye to this fine vessel of words today? Speak up, matey!\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'abc12345'}}\n",
    "\n",
    "query = 'Hi I am Tim.'\n",
    "\n",
    "input_message = HumanMessage(query)\n",
    "\n",
    "output = app.invoke({'messages': [input_message]}, config)\n",
    "    \n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Aye, I be knowin' ye as Tim, me hearty! But the depths of yer soul be a mystery to this ol' sea dog. What tales do ye have to share, or what questions be weighin' on yer mind?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = 'You know who I am?'\n",
    "\n",
    "input_message = HumanMessage(query)\n",
    "\n",
    "output = app.invoke({'messages': [input_message]}, config)\n",
    "    \n",
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complicated prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\n",
    "         'You are a helpful assistant, answer all the question to your best in {language}'),\n",
    "         MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "workflow.add_edge(START, 'model')\n",
    "workflow.add_node('model', call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，Bob！有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'abab123'}}\n",
    "\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_message = HumanMessage(query)\n",
    "\n",
    "output = app.invoke({\n",
    "     'messages': [input_message], 'language': language,\n",
    "}, config)\n",
    "\n",
    "output['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你的名字是Bob。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"what is my name\"\n",
    "\n",
    "input_message = HumanMessage(query)\n",
    "\n",
    "output = app.invoke({\n",
    "     'messages': [input_message]}, config)\n",
    "\n",
    "output['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim message to prevent History gets too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 65,\n",
    "    strategy = 'last',\n",
    "    token_counter = model,\n",
    "    include_system =True,\n",
    "    allow_partial = False,\n",
    "    start_on = 'human'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state['messages'])\n",
    "\n",
    "    prompt = prompt_template.invoke({\n",
    "        'messages': trimmed_messages, 'language': state['language']\n",
    "    })\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "    return {'messages': response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}, id='c942d70f-b089-4604-801b-ad303f4c2ed9'),\n",
       "  HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='9be8f3e6-1ae0-4261-8a40-eb8f89b0fd32'),\n",
       "  AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, id='b1694596-827f-4ed0-a554-167c3b40b17b'),\n",
       "  HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}, id='6b218616-3f8c-49ea-a805-f1e21c2bb97f'),\n",
       "  AIMessage(content='nice', additional_kwargs={}, response_metadata={}, id='51bf760b-6390-4027-a380-995c753bb50e'),\n",
       "  HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}, id='4819760a-0031-481d-bb43-582094ea63bd'),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}, id='8a0879ab-d1b4-428f-ab3c-c5b899767aa8'),\n",
       "  HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}, id='cb6d6c80-dac0-45c9-a8bd-b8a690b4b20c'),\n",
       "  AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}, id='1a7c0b06-312c-4558-ae41-6c62b2e12ae3'),\n",
       "  HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}, id='93e0ec98-0932-4968-80af-1fb1af2ee654'),\n",
       "  AIMessage(content='yes!', additional_kwargs={}, response_metadata={}, id='cb77c73e-249e-4cd9-9e5e-d42777234bf2'),\n",
       "  HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='fdde58a3-3c16-4117-8217-b33ba4b857f4'),\n",
       "  AIMessage(content=\"I don't know your name. You haven't told me yet!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 80, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-a92f5db4-057b-4df4-8ca5-1c7ab8ad1e24-0', usage_metadata={'input_tokens': 80, 'output_tokens': 13, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'language': 'English'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_message = messages + [HumanMessage('what is my name?')]\n",
    "language = 'English'\n",
    "config = {'configurable': {'thread_id': 'aa123'}}\n",
    "output = app.invoke({'messages': input_message,'language' : language }, config)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}, id='c942d70f-b089-4604-801b-ad303f4c2ed9'),\n",
       "  HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='9be8f3e6-1ae0-4261-8a40-eb8f89b0fd32'),\n",
       "  AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, id='b1694596-827f-4ed0-a554-167c3b40b17b'),\n",
       "  HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}, id='6b218616-3f8c-49ea-a805-f1e21c2bb97f'),\n",
       "  AIMessage(content='nice', additional_kwargs={}, response_metadata={}, id='51bf760b-6390-4027-a380-995c753bb50e'),\n",
       "  HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}, id='4819760a-0031-481d-bb43-582094ea63bd'),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}, id='8a0879ab-d1b4-428f-ab3c-c5b899767aa8'),\n",
       "  HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}, id='cb6d6c80-dac0-45c9-a8bd-b8a690b4b20c'),\n",
       "  AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}, id='1a7c0b06-312c-4558-ae41-6c62b2e12ae3'),\n",
       "  HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}, id='93e0ec98-0932-4968-80af-1fb1af2ee654'),\n",
       "  AIMessage(content='yes!', additional_kwargs={}, response_metadata={}, id='cb77c73e-249e-4cd9-9e5e-d42777234bf2'),\n",
       "  HumanMessage(content='what math problem did I ask?', additional_kwargs={}, response_metadata={}, id='9c538342-435f-4ae3-b8ba-9e3c86e20445'),\n",
       "  AIMessage(content='You asked for the sum of 2 + 2.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 82, 'total_tokens': 95, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-70cb9f61-bd3c-4236-ba4a-fc6fdd07e56b-0', usage_metadata={'input_tokens': 82, 'output_tokens': 13, 'total_tokens': 95, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'language': 'English'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_message = messages + [HumanMessage('what math problem did I ask?')]\n",
    "\n",
    "config = {'configurable': {'thread_id': 'aa1234'}}\n",
    "output = app.invoke({'messages': input_message,'language' : language }, config)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$C$inderella$,$ a$ kind$ and$ gentle$ girl$,$ lives$ with$ her$ cruel$ step$mother$ and$ steps$isters$ who$ mist$reat$ her$.$ One$ day$,$ an$ invitation$ to$ a$ royal$ ball$ arrives$,$ and$ her$ steps$isters$ refuse$ to$ let$ her$ attend$.$ With$ the$ help$ of$ her$ fairy$ god$mother$,$ Cinderella$ is$ magically$ transformed$,$ receiving$ a$ beautiful$ gown$ and$ glass$ slippers$.$ She$ attends$ the$ ball$,$ captivating$ the$ prince$,$ but$ must$ leave$ before$ midnight$ when$ the$ magic$ fades$.$ In$ her$ haste$,$ she$ loses$ a$ glass$ slipper$.$ The$ prince$ searches$ for$ her$,$ and$ when$ he$ finds$ her$,$ they$ are$ reunited$.$ Cinderella$'s$ kindness$ prev$ails$,$ and$ they$ live$ happily$ ever$ after$.$$"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage('tell me the story of cinderella in 100 words?')\n",
    "language = 'English'\n",
    "config = {'configurable': {'thread_id': 'aa12345'}}\n",
    "\n",
    "for chunk, metadata in app.stream(\n",
    "    {'messages': input_message, 'language': language},\n",
    "    config,\n",
    "    stream_mode='messages'\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):\n",
    "        print(chunk.content, end='$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Follow tutorial: Retrive - Generate ==> put in a LangGraph object\n",
    "2. Build custom modules manually: without using LangGraph\n",
    "3. Upgrade: change the text splitter to suitable for scientific documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init llm, embeddings, vector_store\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\", ),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43130"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66,\n",
       " [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.')])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits), all_splits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_core.vectorstores.in_memory.InMemoryVectorStore at 0x7e97e462bd00>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(all_splits)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')    # a predefined string used as RAG prompt\n",
    "\n",
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: (question goes here) \\nContext: (describe the context here) \\nAnswer:\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "example_messages = prompt.invoke({\n",
    "    'context': \"(describe the context here)\", \"question\": \"(question goes here)\"\n",
    "}).to_messages()\n",
    "\n",
    "assert len(example_messages) == 1 # why do we need to assert this?\n",
    "\n",
    "print(example_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    context: List[Document]\n",
    "    question: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state['question'])\n",
    "    return {'context': retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state['context'])\n",
    "\n",
    "    messages = prompt.invoke({\n",
    "        'context': docs_content, 'question': state['question']\n",
    "    })\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return {'answer': response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "\n",
    "graph_builder.add_edge(START, 'retrieve')\n",
    "graph = graph_builder.compile()  ## no memory ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(id='554974eb-e6f7-4fa6-afdf-e36ef5505552', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.'),\n",
       "  Document(id='02db1f52-b556-4d44-8eff-a3abdae17abb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.'),\n",
       "  Document(id='3046d6c1-1c0a-439c-a796-4c97859f42eb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.'),\n",
       "  Document(id='ee83077b-f01c-4c35-848e-a6b7c1ac1a4c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)')],\n",
       " 'question': 'What does self-reflection mean?',\n",
       " 'answer': \"Self-reflection refers to the process by which autonomous agents iteratively improve by analyzing and refining their past actions and decisions. It allows agents to learn from mistakes and enhance their performance in real-world tasks that involve trial and error. In this context, self-reflection is facilitated by incorporating reflections into the agent's working memory to guide future actions.\"}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph.invoke({'question': \"What does self-reflection mean?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='6d1177f4-17aa-45bb-aaab-e4ae5573f8d2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.'), Document(id='75dbb351-679f-4846-bb59-1fec801085b3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'), Document(id='ca93cd6f-e915-4957-9162-e3f42d9d8e71', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='4b6855f0-f4d2-469e-a540-1766b96f24a9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:')]}}\n",
      "\n",
      "\n",
      "{'generate': {'answer': 'There are three main components of an agent in a LLM-powered autonomous agent system: planning, memory, and reflection. These components enable the agent to break down tasks, retain past experiences, and refine its actions based on self-criticism. This combination allows the agent to behave intelligently and interact effectively in its environment.'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream, make it more like chatting, avoid user waiting for full sentence.\n",
    "\n",
    "for step in graph.stream(\n",
    "    {'question': \"How many components of an agent are there?\"}, stream_mode='updates'\n",
    "):\n",
    "    print(f\"{step}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$\n",
      "The$$\n",
      " planning$$\n",
      " phase$$\n",
      " involves$$\n",
      " breaking$$\n",
      " down$$\n",
      " complex$$\n",
      " tasks$$\n",
      " into$$\n",
      " manageable$$\n",
      " steps$$\n",
      " and$$\n",
      " form$$\n",
      "ulating$$\n",
      " a$$\n",
      " strategy$$\n",
      " for$$\n",
      " action$$\n",
      ".$$\n",
      " It$$\n",
      " incorporates$$\n",
      " the$$\n",
      " relationships$$\n",
      " between$$\n",
      " agents$$\n",
      " and$$\n",
      " observations$$\n",
      ",$$\n",
      " along$$\n",
      " with$$\n",
      " environmental$$\n",
      " information$$\n",
      " structured$$\n",
      " in$$\n",
      " a$$\n",
      " tree$$\n",
      " format$$\n",
      ".$$\n",
      " This$$\n",
      " phase$$\n",
      " is$$\n",
      " crucial$$\n",
      " for$$\n",
      " optimizing$$\n",
      " the$$\n",
      " agent$$\n",
      "'s$$\n",
      " responses$$\n",
      " and$$\n",
      " improving$$\n",
      " performance$$\n",
      " through$$\n",
      " techniques$$\n",
      " like$$\n",
      " task$$\n",
      " decomposition$$\n",
      " and$$\n",
      " self$$\n",
      "-ref$$\n",
      "lection$$\n",
      ".$$\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "for token in graph.stream(\n",
    "    {'question': \"What is the planning phase?\"}, stream_mode = 'messages'\n",
    "):\n",
    "    print(f\"{token[0].content}\", end='$$\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "  'section': 'beginning'},\n",
       " {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "  'section': 'ending'})"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update metadata of documents, add 'beginining' / 'middle' / 'end' as 'section'\n",
    "\n",
    "len_third = len(all_splits) // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < len_third:\n",
    "        document.metadata['section'] = 'beginning'\n",
    "    elif i < 2*len_third:\n",
    "        document.metadata['section'] = 'middle'\n",
    "    else:\n",
    "        document.metadata['section'] = 'ending'\n",
    "\n",
    "all_splits[0].metadata, all_splits[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "class SearchSection(TypedDict):\n",
    "    \"\"\"Search query\"\"\"\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[Literal['beginning', 'middle', 'ending'],\n",
    "                       ...,\n",
    "                       \"Section to query on.\"\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: SearchSection\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(schema=SearchSection)\n",
    "    query = structured_llm.invoke(state['question'])\n",
    "    return {'query': query}\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state['query']\n",
    "\n",
    "    context = vector_store.similarity_search(query['query'],\n",
    "                                            #  filter={'section': query['section']}) # Custom changed\n",
    "                                            filter = lambda doc: doc.metadata.get('section') == query['section'])\n",
    "    return {'context': context}\n",
    "\n",
    "def generate(state: State):\n",
    "    context = state['context']\n",
    "    question = state['question']\n",
    "\n",
    "    input_messages = prompt.invoke( {'context': context, 'question': question})\n",
    "    response = llm.invoke(input_messages)\n",
    "    return {'answer': response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, 'analyze_query')\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'query': 'context length', 'section': 'ending'}}}\n",
      "-------------------------\n",
      "\n",
      "{'retrieve': {'context': [Document(id='8c758756-f290-4df3-aff5-42e21a5b2932', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'ending'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='0a34819b-d842-40c8-aa84-1e46b11def44', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'ending'}, page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",'), Document(id='cf48a55e-0795-485b-bcb8-3f9cf0a16c8f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'ending'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(id='8f32bc6b-0f26-4330-9c79-56094f669d77', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'ending'}, page_content='Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.')]}}\n",
      "-------------------------\n",
      "\n",
      "{'generate': {'answer': \"The end section discusses that finite context length limits the inclusion of historical information, detailed instructions, and API call context. This restricted capacity hinders the system's ability to learn from past mistakes, which could benefit from longer or infinite context windows. While vector stores can provide access to more knowledge, they lack the representational power of full attention mechanisms.\"}}\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end section says about context length?\"},\n",
    "    stream_mode = 'updates'\n",
    "):\n",
    "    print(f\"{step}\\n-------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call 3 functions one after another, without a state\n",
    "1. ananlyze query: extract from the query whether user want to filter which section [beginning / middle / ending]\n",
    "2. extract: using the `section` and `question` to extract suitable `context` from the data\n",
    "3. generate: compose a grammatically correct answer to give to user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, api_key\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = api_key.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init llm, embeddings, vector_store\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66,\n",
       " [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\", ),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits), all_splits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "class SearchSection(TypedDict):\n",
    "    \"\"\"Search query\"\"\"\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[Literal['beginning', 'middle', 'ending'],\n",
    "                       ...,\n",
    "                       \"Section to query on.\"\n",
    "                       ]\n",
    "    \n",
    "\n",
    "structured_llm = llm.with_structured_output(schema = SearchSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "  'section': 'beginning'},\n",
       " {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "  'section': 'ending'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_third = len(all_splits) // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < len_third:\n",
    "        document.metadata['section'] = 'beginning'\n",
    "    elif i < 2*len_third:\n",
    "        document.metadata['section'] = 'middle'\n",
    "    else:\n",
    "        document.metadata['section'] = 'ending'\n",
    "\n",
    "all_splits[0].metadata, all_splits[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beginning'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0].metadata['section']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a0eb4968-90a8-44ba-b90d-9fd17afadbdc',\n",
       " 'd8d9be9a-b853-424b-9a76-0cc384cea3a0',\n",
       " '32de55e4-28b5-4de1-a3c6-42584609669f',\n",
       " 'c6676fdf-f7ca-4d8b-90ae-ee4f3ac8bce6',\n",
       " 'c14136f9-c36a-42a5-9af8-9bf3abfabfb6',\n",
       " '567cb20f-0296-449f-bb76-58fe6c523f9f',\n",
       " '7cb61978-d87f-43fa-90d6-ae1a1b878c6a',\n",
       " '1fbb277a-32a0-4636-ae29-9ed78efcf8e8',\n",
       " '7c878499-76f8-49de-aa35-e8d2e02e5c45',\n",
       " 'd260081f-e291-4121-8fbf-f549798cef1e',\n",
       " 'fcf42d82-17bb-4965-91e6-9cad4c6cf4f8',\n",
       " '85ecd6f8-d6f3-417e-8fe1-0cd55369d2a1',\n",
       " 'db45a0b5-a4bf-4cc5-a49c-20f9c65af833',\n",
       " '79aadc19-1be6-4fe3-bb27-818195903ef3',\n",
       " '6e9c2ff9-59e9-448e-80ef-1a3d8dc733a8',\n",
       " '43e264cd-8835-49fe-81f1-5aec814c77d7',\n",
       " '357979ed-bb83-43b6-830e-3db76f1fae5a',\n",
       " '746b34a9-1478-4204-ab1f-c3f1e783d8a8',\n",
       " 'e5c05a43-a09c-44b3-b21a-dae4317c3647',\n",
       " 'a351d033-57ca-4695-9330-3e3923c91705',\n",
       " '6f6502e1-72ad-46e1-a1e2-5db5259a721d',\n",
       " '779cded0-9015-4eb0-98f2-e484d185be0a',\n",
       " 'd5aa295b-6ce5-4f7b-bd6c-340c9753ecc7',\n",
       " '15a0e406-7bf0-4341-a19d-1235e23c187f',\n",
       " '86895648-d476-40c7-b34d-a41c6e9824ba',\n",
       " '81541e80-6bc8-4be8-a1ce-730bf8cd4db5',\n",
       " '050a1bbf-107b-4830-96e4-bebeab6455e7',\n",
       " '2506c1ca-bd0c-4b7d-b986-44663b230826',\n",
       " 'fc8d7713-afea-46eb-a104-5e0579b71466',\n",
       " '44f381bc-ec3b-4b83-b174-f68cd6a89449',\n",
       " '85c244a4-5e83-41bb-97ee-35f1c01b8f28',\n",
       " 'b0126f5c-543d-41ab-818e-1b13ea4c75bd',\n",
       " '3ca9ffad-785b-4571-a265-d89510a92be2',\n",
       " 'c3fd46cd-33bb-4c5b-af63-493de09b69f1',\n",
       " 'c0824da4-e243-4347-9210-a445280c9789',\n",
       " 'f5ad833a-3733-4b17-8a3e-d6df0bbddf3f',\n",
       " '6494ad40-26f4-405e-bfd7-15b91c8b4025',\n",
       " '02d7ce2f-e00e-404d-a56b-813356ca42d5',\n",
       " 'f388f482-4626-4c8a-a103-baae0898ed72',\n",
       " '7891fcf5-cdb2-4e5d-a09a-13fde30d970e',\n",
       " '30f60e8f-8ea6-4f5a-a2b0-136b83155a3e',\n",
       " '31df5fa4-39b1-4f68-ae21-d11d2d4f3811',\n",
       " 'a82ba4f8-2cd9-4046-9f72-3f270fb95c2f',\n",
       " 'cd385a9a-e1ff-4fc3-ac1d-92068dc5bdcc',\n",
       " 'b4377fa8-a87b-4657-a8c2-46d7dc304696',\n",
       " 'c18ac028-a372-409f-9efa-088a23b39b07',\n",
       " 'f16b0edb-fac6-4844-b3c8-90eb032a4bae',\n",
       " 'f0fd51dc-dc47-4e68-b200-ffc685fe2919',\n",
       " '7a9ef2e9-d1ff-4cdb-9efc-531ffc4c86b4',\n",
       " 'b8bb1515-938c-429f-84df-dc401510b49c',\n",
       " '7c46b22d-1341-42d6-82ef-34b8df4985fa',\n",
       " 'edec0873-48ac-4960-a91b-a9165ca4fed6',\n",
       " '5a0185c0-0522-4bb7-963c-6766c6f799e6',\n",
       " 'a5d545ed-eb72-42b6-87aa-0ad3ed65ba8c',\n",
       " '5376828d-6434-4b94-9a3c-c379ebdc8595',\n",
       " 'd30a2c2a-fb30-422f-b01d-c444b65a7404',\n",
       " '9d4d3647-a840-4640-8197-0dddf74eed75',\n",
       " '216cc1cf-644e-4e0a-8acf-d159f1be8cf5',\n",
       " 'a9d365b0-d8f0-4d2f-be4c-aef314fd0067',\n",
       " '0f04cce5-3f80-4a4d-bcc1-11b98a51d8c0',\n",
       " 'b27d9ebe-db18-40a5-96d3-2eeb4ee0a7d7',\n",
       " '432e3dff-c6b4-4ee9-b9de-716c1f0e2abe',\n",
       " '72b8ab49-77c8-4a58-ad59-d06b37e61300',\n",
       " '19a67016-83ab-42ff-bacd-4a65e603981e',\n",
       " '2ddd7134-bd18-456c-b3b9-ad0261410495',\n",
       " 'ef26d8f2-a956-4936-bc01-188505c3d411']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "def analyze_query(question):\n",
    "    \n",
    "    desired_section = structured_llm.invoke([HumanMessage(question)])\n",
    "    return desired_section\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        'system',\n",
    "        'You are an expert in answering questions based on the provided context,'\n",
    "         'if you do not know the answer to a question, you can say you dont know.'\n",
    "         'Use maximum 3 sentences to answer, keep your answer concise.'\n",
    "    ),\n",
    "    (\n",
    "        'user',\n",
    "        \"Question: {question} \\nContext: {context}\"\n",
    "    )\n",
    "]) \n",
    "def extract_context(desired_section) -> List[Document]:\n",
    "    query = desired_section['query']\n",
    "    section = desired_section['section']\n",
    "    print(f\"IN extract_context, query: {query}, section: {section}\")\n",
    "    context = vector_store.similarity_search(\n",
    "        query=query,\n",
    "        filter = lambda doc: doc.metadata['section'] == section\n",
    "    )\n",
    "    return context\n",
    "\n",
    "def generate(question, context) -> str:\n",
    "\n",
    "    message_retrieve_context= prompt.invoke(\n",
    "        {\"question\": question, 'context': context})\n",
    "    \n",
    "    response = llm.invoke(message_retrieve_context)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_context, query: challenges when building agents, section: beginning\n",
      "\n",
      "\n",
      "Question: What does the beginning section says about challenges when building agents? \n",
      "Detected section: {'query': 'challenges when building agents', 'section': 'beginning'} \n",
      "Context: [Document(id='a0eb4968-90a8-44ba-b90d-9fd17afadbdc', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'), Document(id='32de55e4-28b5-4de1-a3c6-42584609669f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='c14136f9-c36a-42a5-9af8-9bf3abfabfb6', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.'), Document(id='d8d9be9a-b853-424b-9a76-0cc384cea3a0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.')] \n",
      "Answer: The beginning section highlights that building agents with LLMs as their core involves challenges such as efficient planning and task decomposition. It emphasizes the need for agents to break down complex tasks into manageable subgoals and to engage in self-reflection to improve their decision-making over time. Additionally, leveraging external tools for planning and memory management adds layers of complexity to the development process.\n"
     ]
    }
   ],
   "source": [
    "question = \"What does the beginning section says about challenges when building agents?\"\n",
    "\n",
    "section = analyze_query(question=question)\n",
    "\n",
    "context = extract_context(desired_section=section)\n",
    "\n",
    "response = generate(question=question,\n",
    "                    context=context)\n",
    "\n",
    "print(f\"\\n\\nQuestion: {question} \\nDetected section: {section} \\nContext: {context} \\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test if this text_splitter can do QA on scientific documents (it was recommended only for general documents)\n",
    "- Change to another more suitable for `scientific document`\n",
    "\n",
    "\n",
    "Next:\n",
    "- Add memory\n",
    "- Convert to state? to see if it become simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Lets build a RAG model to retrieve info from this paper\\n        paper: REALM: Retrieval-Augmented Language Model Pre-Training\\n        link: https://arxiv.org/pdf/2002.08909\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Lets build a RAG model to retrieve info from this paper\n",
    "        paper: REALM: Retrieval-Augmented Language Model Pre-Training\n",
    "        link: https://arxiv.org/pdf/2002.08909\n",
    "'''\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = 'https://arxiv.org/pdf/2002.08909'\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 0}, page_content='arXiv:2002.08909v1  [cs.CL]  10 Feb 2020\\nREALM: Retrieval-Augmented Language Model Pre-T raining\\nKelvin Guu * 1 Kenton Lee * 1 Zora T ung1 Panupong Pasupat 1 Ming-W ei Chang1\\nAbstract\\nLanguage model pre-training has been shown to\\ncapture a surprising amount of world knowledge,\\ncrucial for NLP tasks such as question answer-\\ning. However, this knowledge is stored implic-\\nitly in the parameters of a neural network, requir-\\ning ever-larger networks to cover more facts. T o\\ncapture knowledge in a more modular and inter-\\npretable way, we augment language model pre-\\ntraining with a latentknowledge retriever , which\\nallows the model to retrieve and attend over doc-\\numents from a large corpus such as Wikipedia,\\nused during pre-training, ﬁne-tuning and infer-\\nence. For the ﬁrst time, we show how to pre-\\ntrain such a knowledge retriever in an unsuper-\\nvised manner, using masked language model-\\ning as the learning signal and backpropagating\\nthrough a retrieval step that considers millions\\nof documents. W e demonstrate the effective-\\nness of Retrieval-Augmented Language Model\\npre-training (REALM) by ﬁne-tuning on the chal-\\nlenging task of Open-domain Question Answer-\\ning (Open-QA). W e compare against state-of-the-\\nart models for both explicit and implicit knowl-\\nedge storage on three popular Open-QA bench-\\nmarks, and ﬁnd that we outperform all previous\\nmethods by a signiﬁcant margin (4-16% absolute\\naccuracy), while also providing qualitative bene-\\nﬁts such as interpretability and modularity.\\n1. Introduction\\nRecent advances in language model pre-training have\\nshown that models such as BER T (\\nDevlin et al. , 2018),\\nRoBER T a ( Liu et al. , 2019) and T5 ( Raffel et al. , 2019)\\nstore a surprising amount of world knowledge, ac-\\nquired from the massive text corpora they are trained\\non (\\nPetroni et al. , 2019). For example, BER T is able to\\n* Equal contribution 1 Google Research. Correspondence\\nto: Kelvin Guu <kguu@google.com>, Kenton Lee <ken-\\ntonl@google.com>, Zora Tung <gatoatigrado@google.com>,\\nPanupong Pasupat <ppasupat@google.com>, Ming-W ei Chang\\n<mingweichang@google.com>.\\nFigure 1. REALM augments language model pre-training with\\na neural knowledge retriever that retrieves knowledge from a\\ntextual knowledge corpus , Z (e.g., all of Wikipedia). Signal\\nfrom the language modeling objective backpropagates all th e way\\nthrough the retriever, which must consider millions of docu ments\\nin Z—a signiﬁcant computational challenge that we address.\\ncorrectly predict the missing word in the following sen-\\ntence: “The\\nis the currency of the United\\nKingdom” (answer: “ pound”).\\nIn these language models, the learned world knowledge is\\nstoredimplicitly in the parameters of the underlying neural\\nnetwork. This makes it difﬁcult to determine what knowl-\\nedge is stored in the network and where. Furthermore, stor-\\nage space is limited by the size of the network—to cap-\\nture more world knowledge, one must train ever-larger net-\\nworks, which can be prohibitively slow or expensive.\\nT o capture knowledge in a more interpretable and modular\\nway, we propose a novel framework, Retrieval-Augmented\\nLanguage Model (REALM) pre-training, which augments\\nlanguage model pre-training algorithms with a learnedtex-\\ntual knowledge retriever . In contrast to models that store\\nknowledge in their parameters, this approach explicitly ex-\\nposes the role of world knowledge by asking the model to'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\ndecide what knowledge to retrieve and use during inference.\\nBefore making each prediction, the language model uses\\nthe retriever to retrieve documents\\n1 from a large corpus\\nsuch as Wikipedia, and then attends over those documents\\nto help inform its prediction. Learning this model end-to-\\nend requires backpropagating through a retrieval step that\\nconsiders an entire corpus of textual knowledge, as shown\\nin Figure\\n1.\\nThe key intuition of REALM is to train the retriever us-\\ning a performance-based signal from unsupervised text:\\na retrieval that improves the language model’s perplex-\\nity is helpful and should be rewarded, while an un-\\ninformative retrieval should be penalized. For exam-\\nple, in Figure\\n1, if the model needs to ﬁll the blank\\nin “ the at the top of the pyramid”, the re-\\ntriever should be rewarded for selecting a document con-\\ntaining “The pyramidion on top allows for less\\nmaterial higher up the pyramid”. W e achieve this\\nbehavior by modeling our retrieve-then-predict approach\\nas a latent variable language model and optimizing the\\nmarginal likelihood.\\nIncorporating a large-scale neural retrieval module during\\npre-training constitutes a signiﬁcant computational chal -\\nlenge, since the retriever must consider millions of candi-\\ndate documents for each pre-training step, and we must\\nbackpropagate through its decisions. T o address this, we\\nstructure the retriever such that the computation performed\\nfor each document can be cached and asynchronously up-\\ndated, and selection of the best documents can be formu-\\nlated as Maximum Inner Product Search (MIPS).\\nNumerous prior works have demonstrated the bene-\\nﬁt of adding a discrete retrieval step to neural net-\\nworks (\\nMiller et al. , 2016; Chen et al. , 2017), but did not\\napply the framework to language model pre-training and\\nemployed non-learned retrievers to handle large-scale doc-\\nument collections. In the language modeling literature, th e\\nk-Nearest Neighbor Language Model (\\nKhandelwal et al. ,\\n2019) ( kNN-LM) retrieves similar LM examples to im-\\nprove memorization. However, kNN-LM was not ﬁne-\\ntuned for downstream tasks, perhaps because it is unclear\\nhow to adapt the retrieval mechanism: akNN can only use\\nexamples labeled for the target task—during ﬁne-tuning,\\nthis precludes LM examples, which contain the desired\\nworld knowledge. In contrast, REALM’s retriever is de-\\nsigned to transfer to other tasks, and the retrieval is just\\ntext, not a labeled example.\\nW e evaluate our approach by ﬁne-tuning the mod-\\nels pre-trained with REALM on the task of Open-\\ndomain Question Answering (Open-QA), one of the most\\nknowledge-intensive tasks in natural language process-\\ning. W e evaluate on three popular Open-QA bench-\\nmarks (NAT U R A LQU E S T I O N S -O P EN , WE B QU E S T I O N S , and\\nCU R AT E DTR E C) and compare to state-of-the-art Open-QA\\nmodels, including both extremely large models that store\\nknowledge implicitly (such as T5) as well as previous ap-\\nproaches that also use a knowledge retriever to access ex-\\nternal knowledge, but implement retrieval in a more heuris-\\ntic fashion (\\nLee et al. , 2019; Min et al. , 2019a; Asai et al. ,\\n2019). REALM achieves new state-of-the-art results on all\\nthree benchmarks, signiﬁcantly outperforming all previou s\\nsystems by 4-16% absolute accuracy. W e also demonstrate\\nqualitative beneﬁts of REALM, including interpretability\\nand modularity.\\n2. Background\\nLanguage model pre-trainingThe goal of language\\nmodel pre-training is to learn useful representations of la n-\\nguage, usually from unlabeled text corpora. The resulting\\npre-trained model can then be further trained (ﬁne-tuned )\\nfor a downstream task of primary interest (in our case,\\nOpen-QA), often leading to better generalization than train-\\ning from scratch (\\nDai & Le , 2015; Radford et al. , 2019).\\nW e focus on the masked language model 2 (MLM) variant\\nof pre-training popularized by BER T ( Devlin et al. , 2018).\\nIn its basic form, an MLM is trained to predict the miss-\\ning tokens in an input text passage. Given an unlabeled\\npre-training corpusX (e.g., Wikipedia text), a training ex-\\nample (x, y ) can be generated by randomly masking to-\\nkens in a sampled piece of text (e.g., x = “ The [MASK]\\nis the currency [MASK] the UK”; y = (“ pound”,\\n“ of”)). The model uses its representation of the masked\\ninput x to predict the token that should go in each mask.\\nA good MLM must learn to encode syntactic and semantic\\ninformation (e.g., to predict “ of”) as well as some world\\nknowledge (e.g., to predict “ pound”).\\nOpen-domain question answering (Open-QA) T o mea-\\nsure a model’s ability to incorporate world knowledge, we\\nneed a downstream task where world knowledge is criti-\\ncal. Perhaps one of the most knowledge-intensive tasks in\\nnatural language processing is open-domain question an-\\nswering (Open-QA): given a questionx such as “ What is\\nthe currency of the UK?”, a model must output the\\ncorrect answer string y, “ pound”. The “open” part of Open-\\nQA refers to the fact that the model does not receive a pre-\\nidentiﬁed document that is known to contain the answer,\\nunlike traditional reading comprehension (RC) tasks such\\nas SQuAD (\\nRajpurkar et al. , 2016; 2018). While RC mod-\\n1 W e use the term “document” loosely to refer to a passage\\nfrom the knowledge corpus, not necessarily a whole article.\\n2 Strictly speaking, MLM is not a standard language model,\\nsince it does not deﬁne a distribution over the entire sequen ce\\nof tokens. In the paper we sometimes abuse the term “language\\nmodel” slightly to make the phrase shorter.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 2}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\nels comprehend a single document, Open-QA models must\\nretain knowledge from millions of documents, since a ques-\\ntion could be about any of them.\\nW e focus on Open-QA systems that utilize a textual knowl-\\nedge corpus Z as the knowledge source. Many of these\\nsystems employ a retrieval-based approach: given a ques-\\ntion x, retrieve potentially relevant documents z from\\nthe corpus Z, and then extract an answer y from the\\ndocuments (\\nBrill et al. , 2002; Chen et al. , 2017; Lee et al. ,\\n2019). Our approach, REALM, is inspired by this\\nparadigm and extends it to language model pre-training.\\nAlternatively, some recent work has proposed generation-\\nbased systems that apply a sequence-to-sequence model on\\nx to directly generate y token-by-token ( Lewis et al. , 2019;\\nRaffel et al. , 2019). W e will compare against state-of-the-\\nart systems from both paradigms in our experiments.\\n3. Approach\\nW e start by formalizing REALM’s pre-training and ﬁne-\\ntuning tasks as a retrieve-then-predict generative process\\nin Section\\n3.1. Then in Section 3.2, we describe the model\\narchitectures for each component of that process. In Sec-\\ntion\\n3.3, we show how to implement REALM pre-training\\nand ﬁne-tuning by maximizing the likelihood of REALM’s\\ngenerative process. En route, we address important compu-\\ntational challenges, explain why training works, and also\\ndiscuss strategies for injecting useful inductive biases.The\\noverall framework is illustrated in Figure\\n2.\\n3.1. REALM’s generative process\\nFor both pre-training and ﬁne-tuning, REALM takes some\\ninputx and learns a distribution p(y | x) over possible out-\\nputs y. For pre-training, the task is masked language mod-\\neling: x is a sentence from a pre-training corpus X with\\nsome tokens masked out, and the model must predict the\\nvalue of those missing tokens, y. For ﬁne-tuning, the task\\nis Open-QA: x is a question, and y is the answer.\\nREALM decomposes p(y | x) into two steps: retrieve, then\\npredict. Given an input x, we ﬁrst retrieve possibly helpful\\ndocuments z from a knowledge corpus Z. W e model this as\\na sample from the distribution p(z | x). Then, we condition\\non both the retrieved z and the original input x to generate\\nthe output y—modeled as p(y | z, x ). T o obtain the overall\\nlikelihood of generating y, we treat z as a latent variable\\nand marginalize over all possible documents z, yielding\\np(y | x) =\\n∑\\nz∈Z\\np(y | z, x ) p(z | x). (1)\\n3.2. Model architecture\\nW e now describe the two key components: the\\nneural knowledge retriever , which models p(z | x),\\nand the knowledge-augmented encoder , which models\\np(y | z, x ).\\nKnowledge Retriever The retriever is deﬁned using a\\ndense inner product model:\\np(z | x) = exp f(x, z )\\n∑\\nz′ exp f(x, z ′),\\nf(x, z ) =Embedinput(x)⊤Embeddoc(z),\\nwhere Embedinput and Embeddoc are embedding functions\\nthat map x and z respectively to d-dimensional vectors.\\nThe relevance score f(x, z ) between x and z is deﬁned as\\nthe inner product of the vector embeddings. The retrieval\\ndistribution is the softmax over all relevance scores.\\nW e implement the embedding functions using BER T -style\\nTransformers (\\nDevlin et al. , 2018). Following standard\\npractices, we join spans of text by applying wordpiece tok-\\nenization, separating them with[SEP] tokens, preﬁxing a\\n[CLS] token, and appending a ﬁnal [SEP] token.\\njoinBERT(x) =[CLS]x[SEP]\\njoinBERT(x1, x 2) =[CLS]x1[SEP]x2[SEP]\\nAs in\\nDevlin et al. (2018), we pass this into a Transformer,\\nwhich produces one vector for each token, including the\\nvector corresponding to [CLS] which is used as a “pooled”\\nrepresentation of the sequence (denoted BERTCLS). Finally,\\nwe perform a linear projection to reduce the dimensionality\\nof the vector, denoted as a projection matrix W:\\nEmbedinput(x) =WinputBERTCLS(joinBERT(x))\\nEmbeddoc(z) =WdocBERTCLS(joinBERT(ztitle , z body))\\nwhere ztitle is the document’s title and zbody is its body. W e\\nlet θ denote all parameters associated with the retriever,\\nwhich include the Transformer and projection matrices.\\nKnowledge-Augmented Encoder Given an input x and\\na retrieved document z, the knowledge-augmented encoder\\ndeﬁnes p(y | z, x ). W e join x and z into a single sequence\\nthat we feed into a Transformer (distinct from the one used\\nin the retriever). This allows us to perform rich cross-\\nattention betweenx and z before predicting y. See Figure\\n1\\nfor a concrete example.\\nAt this stage, the architectures for pre-training and ﬁne-\\ntuning differ slightly. For the masked language model pre-\\ntraining task, we must predict the original value of each\\n[MASK]token in x. T o do so, we use the same masked'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 3}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\nFigure 2. The overall framework of REALM. Left: Unsupervised pre-training. The knowledge retriever and knowledge-augmented\\nencoder are jointly pre-trained on the unsupervised langua ge modeling task. Right: Supervised ﬁne-tuning. After the parameters of the\\nretriever ( θ) and encoder ( φ) have been pre-trained, they are then ﬁne-tuned on a task of p rimary interest, using supervised examples.\\nlanguage modeling (MLM) loss as in Devlin et al. (2018):\\np(y | z, x ) =\\nJx∏\\nj=1\\np(yj | z, x )\\np(yj | z, x ) ∝ exp\\n(\\nw⊤\\nj BERTMASK(j)(joinBERT(x, z body ))\\n)\\nwhere BERTMASK(j) denotes the Transformer output vector\\ncorresponding to the jth masked token, Jx is the total num-\\nber of [MASK] tokens in x, and wj is a learned word em-\\nbedding for token yj .\\nFor Open-QA ﬁne-tuning, we wish to produce the answer\\nstringy. Following previous reading comprehension work\\n(\\nRajpurkar et al. , 2016; Seo et al. , 2016; Lee et al. , 2016;\\nClark & Gardner , 2017), we will assume that the answer\\ny can be found as a contiguous sequence of tokens in some\\ndocument z. Let S(z, y ) be the set of spans matching y in\\nz. Then we can deﬁne p(y | z, x ) as:\\np(y | z, x ) ∝\\n∑\\ns∈S(z,y)\\nexp\\n(\\nMLP\\n([\\nhSTART(s); hEND(s)\\n]))\\nhSTART(s) = BERTSTART(s)(joinBERT(x, z body )),\\nhEND(s) = BERTEND(s)(joinBERT(x, z body )),\\nwhere BERTSTART(s) and BERTEND(s) denote the Transformer\\noutput vectors corresponding to the start and end tokens of\\nspans, respectively, while MLP denotes a feed-forward neu-\\nral network. W e will let φ denote all parameters associated\\nwith the knowledge-augmented encoder.\\n3.3. T raining\\nFor both pre-training and ﬁne-tuning, we train by maxi-\\nmizing the log-likelihoodlog p(y | x) of the correct out-\\nput y. Since both the knowledge retriever and knowledge-\\naugmented encoder are differentiable neural networks, we\\ncan compute the gradient oflog p(y | x) (deﬁned in Equa-\\ntion\\n1) with respect to the model parameters θ and φ, and\\noptimize using stochastic gradient descent.\\nThe key computational challenge is that the marginal prob-\\nability p(y | x) =∑\\nz∈Z p(y | x, z ) p(z | x) involves a sum-\\nmation over all documents z in the knowledge corpus Z.\\nW e approximate this by instead summing over the top k\\ndocuments with highest probability under p(z | x)—this is\\nreasonable if most documents have near zero probability.\\nEven with this approximation, we still need an efﬁcient way\\nto ﬁnd the topk documents. Note that the ordering of doc-\\numents under p(z | x) is the same as under the relevance\\nscore f(x, z ) = Embedinput(x)⊤Embeddoc(z), which is an\\ninner product. Thus, we can employ Maximum Inner Prod-\\nuct Search (MIPS) algorithms to ﬁnd the approximate topk\\ndocuments, using running time and storage space that scale\\nsub-linearly with the number of documents (\\nRam & Gray ,\\n2012; Shrivastava & Li , 2014; Shen et al. , 2015).\\nT o employ MIPS, we must pre-compute Embeddoc(z) for\\nevery z ∈ Z and construct an efﬁcient search index over\\nthese embeddings. However, this data structure will no\\nlonger be consistent withp(z | x) if the parameters θ of\\nEmbeddoc are later updated. Hence, the search index goes\\n“stale” after every gradient update on θ.\\nOur solution is to “refresh” the index by asynchronously\\nre-embedding and re-indexing all documents every several\\nhundred training steps. The MIPS index is slightly stale be-\\ntween refreshes, but note that it isonly used to select the\\ntop k documents. W e recompute p(z | x) and its gradient,\\nusing the fresh θ, for these top k documents after retriev-\\ning them. In Section\\n4.5, we empirically demonstrate that\\nthis procedure results in stable optimization, provided th at\\nrefreshes happen at a sufﬁciently frequent rate.\\nImplementing asynchronous MIPS refreshesW e asyn-\\nchronously refresh the MIPS index by running two jobs in\\nparallel: a primarytrainer job, which performs gradient\\nupdates on the parameters, and a secondary index builder\\njob, which embeds and indexes the documents. As shown'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 4}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\nFigure 3. REALM pre-training with asynchronous MIPS re-\\nfreshes.\\nbelow , the trainer sends the index builder a snapshot of its\\nparameters,θ′. The trainer then continues to train while the\\nindex builder uses θ′ to construct a new index in the back-\\nground. As soon as the index builder is done, it sends the\\nnew index back to the trainer, and the process repeats.\\nWhile asynchronous refreshes can be used for both pre-\\ntraining and ﬁne-tuning, in our experiments we only use it\\nfor pre-training. For ﬁne-tuning, we just build the MIPS in-\\ndex once (using the pre-trainedθ) for simplicity and do not\\nupdate Embeddoc.\\n3 Note that we still ﬁne-tune Embedinput,\\nso the retrieval function is still updated from the query sid e.\\nWhat does the retriever learn? Since the knowledge re-\\ntrieval of REALM is latent, it is not obvious how the train-\\ning objective encourages meaningful retrievals. Here, we\\nshow how it rewards retrievals that improve prediction ac-\\ncuracy.\\nFor a given queryx and document z, recall that f(x, z ) is\\nthe “relevance score” that the knowledge retriever assigns\\nto documentz. W e can see how a single step of gradient\\ndescent during REALM pre-training alters this score by an-\\nalyzing the gradient with respect to the parameters of the\\nknowledge retriever,θ:\\n∇ log p(y | x) =\\n∑\\nz∈Z\\nr(z)∇f(x, z )\\nr(z) =\\n[ p(y | z, x )\\np(y | x) − 1\\n]\\np(z | x).\\nFor each document z, the gradient encourages the retriever\\nto change the score f(x, z ) by r(z) — increasing if r(z)\\nis positive, and decreasing if negative. The multiplier r(z)\\nis positive if and only if p(y | z, x ) > p (y | x). The term\\np(y | z, x ) is the probability of predicting the correct output\\ny when using document z. The term p(y | x) is the expected\\nvalue of p(y | x, z ) when randomly sampling a document\\nfrom p(z | x). Hence, document z receives a positive up-\\ndate whenever it performs better than expected.\\n3 This works because pre-training already yields a good\\nEmbeddoc function. However, it is possible that refreshing the in-\\ndex would further improve performance.\\n3.4. Injecting inductive biases into pre-training\\nIn the process of developing REALM, we discovered sev-\\neral additional strategies that further guide the model to-\\nwards meaningful retrievals, described below .\\nSalient span masking During REALM pre-training, we\\nwant to focus on examples x that require world knowledge\\nto predict the masked tokens. As explained in Section\\n2,\\nsome MLM spans only require local context. T o focus on\\nproblems that require world knowledge, we masksalient\\nspans such as “ United Kingdom” or “ July 1969”. W e\\nuse a BER T -based tagger trained on CoNLL-2003 data\\n(\\nSang & De Meulder , 2003) to identify named entities, and\\na regular expression to identify dates. W e select and mask\\none of these salient spans within a sentence for the masked\\nlanguage modeling task. W e show that this signiﬁcantly\\noutperforms other masking strategies in Section\\n4.5.\\nNull document Even with salient span masking, not all\\nmasked tokens require world knowledge to predict. W e\\nmodel this by adding an emptynull document ∅ to the top\\nk retrieved documents, allowing appropriate credit to be as-\\nsigned to a consistent sink when no retrieval is necessary.\\nProhibiting trivial retrievalsIf the pre-training corpus\\nX and the knowledge corpus Z are the same, there exists\\na trivial retrieval candidate z that is too informative: if the\\nmasked sentence x comes from document z, the knowledge\\naugmented encoder can trivially predict y by looking at the\\nunmasked version of x in z. This results in a large positive\\ngradient for p(z | x). If this occurs too often, the knowledge\\nretriever ends up learning to look for exact string matches\\nbetweenx and z, which does not capture other forms of\\nrelevance. For this reason, we exclude this trivial candida te\\nduring pre-training.\\nInitializationAt the beginning of training, if the retriever\\ndoes not have good embeddings for Embedinput(x) and\\nEmbeddoc(z), the retrieved documents z will likely be unre-\\nlated to x. This causes the knowledge augmented encoder\\nto learn to ignore the retrieved documents. Once this oc-\\ncurs, the knowledge retriever does not receive a meaning-\\nful gradient and cannot improve, creating a vicious cycle.\\nT o avoid this cold-start problem, we warm-start Embedinput\\nand Embeddoc using a simple training objective known as\\nthe Inverse Cloze T ask (ICT) where, given a sentence, the\\nmodel is trained to retrieve the document where that sen-\\ntence came from. W e defer to\\nLee et al. (2019) for de-\\ntails. For the knowledge-augmented encoder, we warm-\\nstart it with BER T pre-training—speciﬁcally, the uncased\\nBER T -base model (12 layers, 768 hidden units, 12 atten-\\ntion heads).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 5}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\n4. Experiments\\nW e now evaluate our approach on the Open-QA task. In\\nthis section, we describe in detail the benchmarks used and\\nthe different approaches to which we compare empirically.\\n4.1. Open-QA Benchmarks\\nA number of benchmarks have been proposed for Open-\\nQA. In this work, we focus on datasets where the ques-\\ntion writers did not already know the answer. This yields\\nquestions that reﬂect more realistic information-seeking\\nneeds, and also avoids artifacts that can arise if the ques-\\ntion is formulated with a particular answer in mind. A\\ndeeper justiﬁcation is given in\\nLee et al. (2019). In all\\ncases, the predicted answer is evaluated via exact match\\nwith any reference answer, following previous Open-QA\\nwork (\\nChen et al. , 2017).\\nNaturalQuestions-Open The NaturalQuestions dataset\\n(Kwiatkowski et al. , 2019) consists of naturally occurring\\nGoogle queries and their answers. Each answer also comes\\nwith an “answer type”: following Lee et al. (2019), we only\\nkeep questions that are categorized as “short answer type”\\nwith at most ﬁve tokens. The dataset also provides a sug-\\ngested Wikipedia document to retrieve; like all models we\\ncompare against, we do not provide this to our model.\\nW ebQuestionsThe W ebQuestions dataset (\\nBerant et al. ,\\n2013) was collected from the Google Suggest API, using\\none seed question and expanding the set to related ques-\\ntions. W e follow the setting deﬁned by\\nChen et al. (2017).\\nCuratedT rec The CuratedTrec dataset is a collection of\\nquestion-answer pairs drawn from real user queries issued\\non sites such as MSNSearch and AskJeeves. T o account for\\nmultiple correct answers or different spelling variations, the\\nanswers in this dataset are deﬁned as regular expressions\\nthat match all correct answers. It is unclear how to train\\ngeneration-based models with this type of supervision, so\\nwe do not evaluate them on this dataset.\\n4.2. Approaches compared\\nRetrieval-based Open-QAMost existing Open-QA sys-\\ntems answer the input question by ﬁrst retrieving poten-\\ntially relevant documents from a knowledge corpus, and\\nthen using a reading comprehension system to extract an\\nanswer from the documents. In this paradigm, the knowl-\\nedge is storedexplicitly in the corpus. W e wish to compare\\ndifferent methods for implementing retrieval.\\nMany approaches use non-learned heuristic retrieval such\\nas sparse bag-of-words matching (\\nRobertson et al. , 2009)\\nor entity linking on the question to select a small set of rel-\\nevant documents (e.g., 20). These documents are typically\\nthen re-ranked using a learned model, but coverage may be\\nlimited by the initial heuristic retrieval step. Approaches\\nsuch as DrQA (\\nChen et al. , 2017), HardEM ( Min et al. ,\\n2019a), GraphRetriever ( Min et al. , 2019b), and PathRe-\\ntriever ( Asai et al. , 2019) in T able 1 are in this category.\\nSome recent approaches have proposed to implement learn-\\nable retrieval using a MIPS index. ORQA (\\nLee et al. , 2019)\\nformulates Open-QA using a similar latent variable model\\nas REALM, and also trains by maximizing the marginal\\nlikelihood. However, REALM adds a novel language\\nmodel pre-training step, and backpropagates into the MIPS\\nindex, rather than using a ﬁxed index. In T able\\n1, we di-\\nrectly compare the two. It is also important to note that\\nthe retrievers for both REALM pretraining and ORQA are\\ninitialized using the Inverse Cloze T ask, described in Sec-\\ntion\\n3.4.\\nGeneration-based Open-QA An emerging alternative\\napproach to Open-QA is to model it as a sequence pre-\\ndiction task: simply encode the question, and then decode\\nthe answer token-by-token based on the encoding. While\\nit was initially unclear how large amounts of knowledge\\ncould be injected into the model, GPT -2 (\\nRadford et al. ,\\n2019) hinted at the possibility of directly generating an-\\nswers without using any given context via sequence-to-\\nsequence. However, their performance was not competi-\\ntive possibly due to the lack of ﬁne-tuning. Orthogonally,\\nT5 (\\nRaffel et al. , 2019) showed that directly generating an-\\nswers without explicit extraction from the given context is\\nviable approach, but they only experimented on the read-\\ning comprehension task, where a context document is pro-\\nvided.\\nFor the most competitive and comparable generation-based\\nbaseline, we compare to concurrent work which ﬁne-tunes\\nT5 for Open-QA (\\nRoberts et al. , 2020).4 W e compare\\nagainst the Base, Large, and even larger 11-billion parame-\\nter model to measure the effect of model size.\\n4.3. Implementation Details\\nFine-tuningW e reuse all hyperparameters from\\nLee et al. (2019), to enable direct comparison. Our\\nknowledge corpus is derived from the December 20, 2018\\nsnapshot of English Wikipedia. Documents are greedily\\nsplit into chunks of up to 288 BER T wordpieces, resulting\\nin just over 13 million retrieval candidates. During ﬁne-\\ntuning inference, we consider the top-5 candidates, and the\\n4 W e initially conducted our own T5 experiments using\\nthe code from https://tinyurl.com/t5-openqa-colab (Raffel et al. ,\\n2019). W e now report results from the concurrent work of\\nRoberts et al. (2020), which has an improved ﬁne-tuning proce-\\ndure.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 6}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\nT able 1. T est results on Open-QA benchmarks. The number of train/tes t examples are shown in paretheses below each benchmark.\\nPredictions are evaluated with exact match against any refe rence answer. Sparse retrieval denotes methods that use spa rse features such\\nas TF-IDF and BM25. Our model, REALM, outperforms all existi ng systems.\\nName Architectures Pre-training NQ\\n(79k/4k)\\nWQ\\n(3k/2k)\\nCT\\n(1k /1k) # params\\nBERT -Baseline (Lee et al. , 2019) Sparse Retr. +Transformer BERT 26.5 17.7 21.3 110m\\nT5 (base) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 27.0 29.1 - 223m\\nT5 (large) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 29.8 32.2 - 738m\\nT5 (11b) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 34.5 37.4 - 11318m\\nDrQA ( Chen et al. , 2017) Sparse Retr. +DocReader N/A - 20.7 25.7 34m\\nHardEM ( Min et al. , 2019a) Sparse Retr. +Transformer BERT 28.1 - - 110m\\nGraphRetriever ( Min et al. , 2019b) GraphRetriever +Transformer BERT 31.8 31.6 - 110m\\nPathRetriever ( Asai et al. , 2019) PathRetriever +Transformer MLM 32.6 - - 110m\\nORQA ( Lee et al. , 2019) Dense Retr. +Transformer ICT +BERT 33.3 36.4 30.1 330m\\nOurs ( X = Wikipedia, Z = Wikipedia) Dense Retr. +Transformer REALM 39.2 40.2 46.8 330m\\nOurs ( X = CC-News, Z = Wikipedia) Dense Retr. +Transformer REALM 40.4 40.7 42.9 330m\\nT able 2. Ablation experiments on NQ’s development set.\\nAblation Exact\\nMatch\\nZero-shot\\nRetrieval\\nRecall@5\\nREALM 38.2 38.5\\nREALM retriever +Baseline encoder 37.4 38.5\\nBaseline retriever +REALM encoder 35.3 13.9\\nBaseline (ORQA) 31.3 13.9\\nREALM with random uniform masks 32.3 24.2\\nREALM with random span masks 35.3 26.1\\n30× stale MIPS 28.7 15.1\\nentire model can be run on a single machine with a 12GB\\nGPU.\\nPre-trainingW e pre-train for 200k steps on 64 Google\\nCloud TPUs, with a batch size of 512 and a learning rate\\nof 3e-5, using BER T’s default optimizer. The document\\nembedding step for the MIPS index is parallelized over 16\\nTPUs. For each example, we retrieve and marginalize over\\n8 candidate documents, including the null document ∅ .\\nW e experiment with two choices of the pre-training corpus\\nX : (1) Wikipedia, which is identical to the knowledge cor-\\npus Z, and (2) CC-News, our reproduction of the corpus of\\nEnglish news proposed by\\nLiu et al. (2019).\\n4.4. Main results\\nT able 1 shows the accuracy of different approaches on the\\nthree Open-QA datasets. REALM outperform all previous\\napproaches by a signiﬁcant margin. T able\\n1 also shows the\\nnumber of parameters for each model.\\nAs reported in the concurrent work of Roberts et al. (2020),\\nthe generative Open-QA systems based on T5 are surpris-\\ningly powerful, with the largest T5-11B model outperform-\\ning the previous best Open-QA system. Increasing the size\\nof T5 yields consistent improvement, but comes at signif-\\nicant computational cost (from Base to 11B, the model is\\n50 times larger, and gains roughly 5 points in accuracy). In\\ncontrast, REALM outperforms the largest T5-11B model\\nwhile being 30 times smaller. It is also important to note\\nthat T5 accesses additional reading comprehension data\\nfrom SQuAD during its pre-training (100,000+ examples).\\nAccess to such data could also beneﬁt REALM, but was not\\nused in our experiments.\\nAmong all systems, the most direct comparison with\\nREALM is ORQA (\\nLee et al. , 2019), where the ﬁne-tuning\\nsetup, hyperparameters and training data are identical. Th e\\nimprovement of REALM over ORQA is purely due to bet-\\nter pre-training methods. The results also indicate that our\\nmethod of pre-training can be applied both on (1) the single-\\ncorpus setting (X = Wikipedia, Z = Wikipedia), or (2) the\\nseparate-corpus setting ( X = CC-News, Z = Wikipedia).\\nCompared to other retrieval-based systems (\\nAsai et al. ,\\n2019; Min et al. , 2019a;b) which often retrieve from 20 to\\n80 documents, our system gets the overall best performance\\nwhile only retrieving 5 documents.\\n4.5. Analysis\\nIn T able\\n2 we present results for NaturalQuestions-Open\\nafter ablating critical components of REALM. In addition\\nto the end-to-end results, we also report how often the gold\\nanswer appears in the top-5 retrievals before applying any\\nﬁne-tuning. The latter metric more signiﬁcantly isolates the\\ncontribution of improving the retriever during pre-traini ng.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 7}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\nT able 3. An example where REALM utilizes retrieved documents to bett er predict masked tokens. It assigns much higher probabilit y\\n(0.129) to the correct term, “ Fermat”, compared to BERT . (Note that the blank corresponds to 3 BER T wordpieces.)\\nx: An equilateral triangle is easily constructed using a stra ightedge and compass, because 3 is a prime.\\n(a) BERT p(y = “ Fermat” |x) = 1 . 1 × 10− 14 (No retrieval.)\\n(b) REALM p(y = “ Fermat” |x, z ) = 1. 0 (Conditional probability with document z =“257 is . . . a Fermat prime.\\nThus a regular polygon with 257 sides is constructible with c ompass . . . ”)\\n(c) REALM p(y = “ Fermat” |x) = 0 . 129 (Marginal probability , marginalizing over top 8 retrieved documents.)\\nEncoder or Retriever W e ﬁrst aim to determine whether\\nREALM pre-training improves the retriever or the encoder,\\nor both. T o do so, we can reset the parameters of either\\nthe retriever or the encoder to their baseline state before\\nREALM pre-training, and feed that into ﬁne-tuning. Reset-\\nting both the retriever and encoder reduces the system to\\nour main baseline, ORQA. W e ﬁnd that both the encoder\\nand retriever beneﬁt from REALM training separately, but\\nthe best result requires both components acting in unison.\\nMasking schemeW e compare our salient span masking\\nscheme (Section\\n3.4) with (1) random token masking in-\\ntroduced in BER T ( Devlin et al. , 2018) and (2) random\\nspan masking proposed by SpanBER T ( Joshi et al. , 2019).\\nWhile such salient span masking has not been shown to\\nbe impactful in previous work with standard BER T train-\\ning (\\nJoshi et al. , 2019), it is crucial for REALM. Intuitively,\\nthe latent variable learning relies heavily on the utility o f re-\\ntrieval and is therefore more sensitive to a consistent lear n-\\ning signal.\\nMIPS index refresh rateDuring pre-training, we run a\\nparallel process to re-embed corpus documents and rebuild\\nthe MIPS index. This results in one index refresh per ap-\\nproximately 500 training steps. T o demonstrate the impor-\\ntance of frequent index refreshes, we compare against using\\na slower refresh rate. The results in T able\\n2 suggests that\\na stale index can hurt model training, and further reducing\\nthis staleness could offer better optimization.\\nExamples of retrieved documentsT able\\n3 shows an\\nexample of the REALM masked language model predic-\\ntion. In this example, “Fermat” is the correct word, and\\nREALM (row (c)) gives the word a much high probability\\ncompared to the BER T model (row (a)). Since REALM\\nmanages to retrieve some documents with a related fact\\n(row (b)), the marginalized probability of the correct an-\\nswer dramatically increases. This shows that REALM is\\nable to retrieve document to ﬁll in the masked word even\\nthough it is trained with unsupervised text only.\\n5. Discussion and Related W ork\\nW e previously discussed related methods for Open-QA.\\nHere we present several alternate ways of viewing REALM\\nthat connect it to a broader set of ideas beyond Open-QA:\\nLanguage modeling with corpus as contextLanguage\\nrepresentation models have been incorporating contexts of\\nincreasingly large scope when making predictions. Ex-\\namples of this progression include models that condi-\\ntion on surrounding words (\\nMikolov et al. , 2013a;b), sen-\\ntences ( Kiros et al. , 2015; Peters et al. , 2018), and para-\\ngraphs ( Radford et al. , 2018; Devlin et al. , 2018). W e can\\nview REALM as a generalization of the above work to the\\nnext level of scope: the entire text corpus.\\nRetrieve-and-edit with learned retrieval In order to\\nbetter explain the variance in the input text and en-\\nable controllable generation,\\nGuu et al. (2018) proposed\\na language model with the retrieve-and-edit frame-\\nwork ( Hashimoto et al. , 2018) that conditions on text with\\nhigh lexical overlap. REALM has a similar approach, ex-\\ncept that the model learns for itself which texts are most\\nuseful for reducing perplexity. By jointly learning the re-\\ntriever, REALM has the capacity to depend on information\\nbeyond lexical overlap.\\nScalable grounded neural memoryThe document in-\\ndex can be viewed as a memory where the keys are\\nthe document embeddings. From this view , our work\\nshare motivations with works such as product key mem-\\nory (\\nLample et al. , 2019), which enables sub-linear mem-\\nory access in a memory network ( W eston et al. , 2014;\\nGraves et al. , 2014; Sukhbaatar et al. , 2015), allowing\\nthese scalable memory layers to be integrated into large\\nlanguage models. One main difference is that our memo-\\nries are grounded—each memory is associated with a docu-\\nment rather than unnamed value vectors. This level of inter-\\npretability is crucial for applications like Open-QA, where\\nusers would require provenance for a predicted answer to\\nbe trustworthy.\\nUnsupervised Corpus AlignmentIn sequence-to-\\nsequence models with attention (\\nBahdanau et al. , 2014),'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 8}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\ntext is generated with latent selection of relevant tokens.\\nThis results in a set of model-centric unsupervised align-\\nments between target and source tokens. Analogously,\\nREALM also generates text with latent selection of\\nrelevant documents. A by-product of our method is that\\nwe offer a set of model-centric unsupervised alignments\\nbetween text in the pre-training corpus X and knowledge\\ncorpus Z.\\n6. Future W ork\\nThe work presented here is the minimal instantiation of a\\nfamily of REALM-like approaches where a representation\\nis pre-trained to perform reasoning over a large corpus of\\nknowledge on-the-ﬂy during inference. W e are particularly\\noptimistic about generalizations of this work to (1) struc-\\ntured knowledge, which would result in a generalization of\\nPeters et al. (2019) where we would also learn the decision\\nof which entities are informative, (2) the multi-lingual se t-\\nting, e.g., retrieving knowledge in a high-resource langua ge\\nto better represent text in a low-resource language, and (3)\\nthe multi-modal setting, e.g., retrieving images or videos\\nthat can provide knowledge rarely observed in text.\\nReferences\\nAsai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and\\nXiong, C. Learning to retrieve reasoning paths over\\nwikipedia graph for question answering.arXiv preprint\\narXiv:1911.10470 , 2019.\\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\\ntranslation by jointly learning to align and translate.\\narXiv preprint arXiv:1409.0473, 2014.\\nBerant, J., Chou, A., Frostig, R., and Liang, P . Semantic\\nparsing on freebase from question-answer pairs. In Pro-\\nceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pp. 1533–1544, 2013.\\nBrill, E., Dumais, S., and Banko, M. An analysis of the\\naskmsr question-answering system. In Empirical Meth-\\nods in Natural Language Processing , 2002.\\nChen, D., Fisch, A., W eston, J., and Bordes, A. Read-\\ning wikipedia to answer open-domain questions. In Pro-\\nceedings of the 55th Annual Meeting of the Association\\nfor Computational Linguistics (V olume 1: Long P apers),\\nvolume 1, pp. 1870–1879, 2017.\\nClark, C. and Gardner, M. Simple and effective multi-\\nparagraph reading comprehension. In Annual Meeting\\nof the Association for Computational Linguistics , 2017.\\nDai, A. M. and Le, Q. V . Semi-supervised sequence learn-\\ning. In Advances in neural information processing sys-\\ntems, pp. 3079–3087, 2015.\\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding.arXiv preprint arXiv:1810.04805 ,\\n2018.\\nGraves, A., W ayne, G., and Danihelka, I. Neural turing\\nmachines. ArXiv, abs/1410.5401, 2014.\\nGuu, K., Hashimoto, T . B., Oren, Y ., and Liang, P . Gen-\\nerating sentences by editing prototypes. T ransactions\\nof the Association for Computational Linguistics , 6:437–\\n450, 2018.\\nHashimoto, T . B., Guu, K., Oren, Y ., and Liang, P . S.\\nA retrieve-and-edit framework for predicting structured\\noutputs. In Advances in Neural Information Processing\\nSystems, pp. 10052–10062, 2018.\\nJoshi, M., Chen, D., Liu, Y ., W eld, D. S., Zettlemoyer,\\nL., and Levy, O. SpanBER T: Improving pre-training\\nby representing and predicting spans.arXiv preprint\\narXiv:1907.10529 , 2019.\\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer,\\nL., and Lewis, M. Generalization through memo-\\nrization: Nearest neighbor language models.ArXiv,\\nabs/1911.00172, 2019.\\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urta-\\nsun, R., T orralba, A., and Fidler, S. Skip-thought vectors.\\nInAdvances in neural information processing systems ,\\npp. 3294–3302, 2015.\\nKwiatkowski, T ., Palomaki, J., Rhinehart, O., Collins, M.,\\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel-\\ncey, M., Devlin, J., et al. Natural questions: a benchmark\\nfor question answering research.T ransactions of the As-\\nsociation for Computational Linguistics , 2019.\\nLample, G., Sablayrolles, A., Ranzato, M., Denoyer, L.,\\nand J´ egou, H. Large memory layers with product keys.\\nInAdvances in Neural Information Processing Systems ,\\npp. 8546–8557, 2019.\\nLee, K., Salant, S., Kwiatkowski, T ., Parikh, A., Das,\\nD., and Berant, J. Learning recurrent span representa-\\ntions for extractive question answering.arXiv preprint\\narXiv:1611.01436 , 2016.\\nLee, K., Chang, M.-W ., and T outanova, K. Latent re-\\ntrieval for weakly supervised open domain question an-\\nswering. InProceedings of the Conference of Associa-\\ntion for Computational Linguistics , 2019.\\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo-\\nhamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L.\\nBart: Denoising sequence-to-sequence pre-training for\\nnatural language generation, translation, and comprehen-\\nsion.ArXiv, abs/1910.13461, 2019.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 9}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\\nRoberta: A robustly optimized bert pretraining approach.\\narXiv preprint arXiv:1907.11692, 2019.\\nMikolov, T ., Chen, K., Corrado, G., and Dean, J. Efﬁcient\\nestimation of word representations in vector space. arXiv\\npreprint arXiv:1301.3781 , 2013a.\\nMikolov, T ., Sutskever, I., Chen, K., Corrado, G. S.,\\nand Dean, J. Distributed representations of words and\\nphrases and their compositionality. InAdvances in\\nneural information processing systems , pp. 3111–3119,\\n2013b.\\nMiller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A.,\\nand W eston, J. Key-value memory networks for directly\\nreading documents.arXiv preprint arXiv:1606.03126 ,\\n2016.\\nMin, S., Chen, D., Hajishirzi, H., and Zettlemoyer, L. A dis-\\ncrete hard em approach for weakly supervised question\\nanswering.arXiv preprint arXiv:1909.04849 , 2019a.\\nMin, S., Chen, D., Zettlemoyer, L., and Hajishirzi,\\nH. Knowledge guided text retrieval and reading\\nfor open domain question answering.arXiv preprint\\narXiv:1911.03868 , 2019b.\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\\nword representations. In Proc. of NAACL , 2018.\\nPeters, M. E., Neumann, M., IV , R. L. L., Schwartz, R.,\\nJoshi, V ., Singh, S., and Smith, N. A. Knowledge en-\\nhanced contextual word representations, 2019.\\nPetroni, F ., Rockt¨ aschel, T ., Lewis, P ., Bakhtin, A., Wu, Y .,\\nMiller, A. H., and Riedel, S. Language models as knowl-\\nedge bases?arXiv preprint arXiv:1909.01066 , 2019.\\nRadford, A., Narasimhan, K., Salimans, T ., and Sutskever,\\nI. Improving language understanding with unsupervised\\nlearning. T echnical report, OpenAI, 2018.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\\nSutskever, I. Language models are unsupervised multi-\\ntask learners.OpenAI Blog , 2019.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y ., Li, W ., and Liu, P . J. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-text\\ntransformer.arXiv preprint arXiv:1910.10683 , 2019.\\nRajpurkar, P ., Zhang, J., Lopyrev, K., and Liang, P . Squad:\\n100,000+ questions for machine comprehension of text.\\nIn Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing , pp. 2383–\\n2392, 2016.\\nRajpurkar, P ., Jia, R., and Liang, P . Know what you don’t\\nknow: Unanswerable questions for squad. arXiv preprint\\narXiv:1806.03822 , 2018.\\nRam, P . and Gray, A. G. Maximum inner-product search us-\\ning cone trees. In Proceedings of the 18th ACM SIGKDD\\ninternational conference on Knowledge discovery and\\ndata mining, pp. 931–939, 2012.\\nRoberts, A., Raffel, C., and Shazeer, N. How much knowl-\\nedge can you pack into the parameters of a language\\nmodel?arXiv preprint arXiv:TBD , 2020.\\nRobertson, S., Zaragoza, H., et al. The probabilistic rele-\\nvance framework: Bm25 and beyond. F oundations and\\nT rends in Information Retrieval , 3(4):333–389, 2009.\\nSang, E. T . K. and De Meulder, F . Introduction to the conll-\\n2003 shared task: Language-independent named entity\\nrecognition. InProceedings of the Seventh Conference\\non Natural Language Learning at HLT -NAACL 2003 , pp.\\n142–147, 2003.\\nSeo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H.\\nBidirectional attention ﬂow for machine comprehension.\\nInInternational Conference on Learning Representa-\\ntions, 2016.\\nShen, F ., Liu, W ., Zhang, S., Y ang, Y ., and T ao Shen,\\nH. Learning binary codes for maximum inner product\\nsearch. InProceedings of the IEEE International Con-\\nference on Computer V ision , pp. 4148–4156, 2015.\\nShrivastava, A. and Li, P . Asymmetric lsh (alsh) for sub-\\nlinear time maximum inner product search (mips). In\\nAdvances in Neural Information Processing Systems , pp.\\n2321–2329, 2014.\\nSukhbaatar, S., W eston, J., Fergus, R., et al. End-to-end\\nmemory networks. In Advances in neural information\\nprocessing systems , 2015.\\nW eston, J., Chopra, S., and Bordes, A. Memory networks.\\narXiv preprint arXiv:1410.3916 , 2014.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 10}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\nA. Derivation of the gradient with respect to\\nthe knowledge retriever\\nW e compute the gradient of the REALM pre-training objec-\\ntive (a log-likelihood) with respect to the parameters of th e\\nknowledge retriever, θ:\\n∇ log p(y | x) =p(y | x)−1∇p(y | x)\\n= p(y | x)−1 ∑\\nz\\np(y | z, x )∇p(z | x)\\n= p(y | x)−1 ∑\\nz\\np(y | z, x )p(z | x)∇ log p(z | x)\\n=\\n∑\\nz\\np(z | y, x )∇ log p(z | x),\\nwhere the last line follows from applying conditional\\nBayes’ rule. W e can then expand ∇ log p (z | x) as:\\n∇ log p(z | x) =∇ log exp f(x, z )∑\\nz′ exp f(x, z ′)\\n= ∇\\n[\\nf(x, z ) − log\\n∑\\nz′\\nexp f(x, z ′)\\n]\\n= ∇f(x, z ) −\\n∑\\nz′\\np(z′ | x)∇f(x, z ′)\\nPlugging this back into the ﬁrst set of equations yields:\\n∇ log p (y | x) =\\n∑\\nz\\np (z | y, x )\\n[\\n∇f(x, z ) −\\n∑\\nz′\\np (z′ | x) ∇f(x, z ′)\\n]\\n=\\n∑\\nz\\np (z | y, x ) ∇f(x, z ) −\\n∑\\nz′\\np (z′ | x) ∇f(x, z ′)\\n=\\n∑\\nz\\n[p (z | y, x ) − p (z | x)] ∇f(x, z )\\n=\\n∑\\nz\\n[ p (y | z, x ) p (z | x)\\np (y | x) − p (z | x)\\n]\\n∇f(x, z )\\n=\\n∑\\nz\\n[ p (y | z, x )\\np (y | x) − 1\\n]\\np (z | x) ∇f(x, z ).\\nIn the second line, we used the fact that the overall expres-\\nsion is an expectation with respect top (z | y, x ), and the\\nterms which depend on z′ but not z can be moved out of\\nthat expectation.\\nB. Connection between REALM and\\nsupervised learning\\nFrom the equations in Appendix A, we saw that\\n∇ log p (y | x) =\\n∑\\nz\\n[p (z | y, x ) − p (z | x)] ∇f(x, z ).\\nSuppose that there exists one document z∗ which causes\\nthe model to achieve perfect prediction accuracy (i.e.,\\np(y | z∗, x ) = 1), while all other documents z′ result in\\nzero accuracy (i.e., p (y | z′, x ) = 0 ). Under this set-\\nting, p (z∗ | y, x ) = 1(provided that p (z∗ | x) is non-zero),\\nwhich causes the gradient to become\\n∇ log p (y | x) =∇f (x, z ∗) −\\n∑\\nz\\np (z | x) ∇f(x, z )\\n= ∇ log p (z∗ | x) .\\nFrom this, we see that gradient descent on the REALM ob-\\njective is equivalent to gradient descent onlog p (z∗ | x).\\nThis is none other than the typical maximum likelihood\\ntraining objective used in supervised learning, where z∗ is\\nthe “gold” document.\\nC. Adapting to new knowledge\\nAn explicit retrieval system allows us to adapt to new\\nworld knowledge simply by modifying the corpus docu-\\nments. T o demonstrate this ability, we replace the knowl-\\nedge corpus with a more recent version of Wikipedia cor-\\npus after pre-training is done. When the input query is\\nabout a fact where the two corpora disagree, REALM can\\nchange the prediction to reﬂect the updated information,\\nas exempliﬁed in T able\\n4. However, even with an ex-\\nplicit retrieval mechanism, the knowledge-augmented en-\\ncoder will still end up remembering some world knowl-\\nedge, making the prediction of some input sentences not\\nupdated with the new corpus. (For instance, the model pre-\\ndicts “Thatcher” for “\\nis the prime minister\\nof United Kingdom.” on both corpora, perhaps due to\\nthe frequent mention of her name in Wikipedia articles.)\\nD. Retrieval Utility\\nThe null document ∅ described in Section\\n3.4 provides a\\nway to measure the importance of a retrieved document z:\\nwe deﬁne the retrieval utility (RU) of z for the masked\\ninput x as the difference between the log-likelihood of\\nthe knowledge-augmented encoder when conditioning on\\nzversus on ∅ :\\nRU(z | x) = logp(y | z, x ) − log p(y | ∅ , x ). (2)\\nA negative RU shows that z is less useful for predicting y\\nthan the null document. This could mean that z is irrelevant\\nto x, but could also mean that the masked tokens in x do\\nnot require world knowledge to predict, or that the world\\nknowledge is sufﬁciently commonplace it has been baked\\ninto the model’s parameters. In practice, we ﬁnd that RU\\nincreases steadily over the course of pre-training, and is\\nmore predictive of good performance on the downstream\\ntask of Open-QA than even the overall log-likelihood. An\\nexample of how RU behaves over time and across different\\nsettings is in Figure\\n4.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 11}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\nx: “ Jennifer formed the production company Excellent Cadaver.”\\nBER T also (0.13), then (0.08), later (0.05), . . .\\nREALM (Z =20 Dec 2018 corpus) smith (0.01), brown (0.01), jones (0.01 )\\nREALM ( Z =20 Jan 2020 corpus) lawrence (0.13), brown (0.01), smith (0.01), . . .\\nT able 4. An example where REALM adapts to the updated knowledge corpu s. The Wikipedia page “Excellent Cadaver” was added in\\n2019, so the model was not about to recover the word when the kn owledge corpus is outdated (2018). Interestingly , the same REALM\\nmodel pre-trained on the 2018 corpus is able to retrieve the d ocument in the updated corpus (2020) and generate the correc t token,\\n“ Lawrence”.\\n0 50 100 150 200\\n0\\n1\\n2\\n3\\nPre-training Steps (Thousands)\\nRetrieval Utility\\nSalient span masking\\nRandom span masking\\nRandom uniform masking\\nFigure 4. The Retrieval Utility (RU, described in Eq. 2) vs the number of pre-training steps. RU roughly estimates t he “usefulness” of\\nretrieval. RU is impacted by the choice of masking and the num ber of pre-training steps.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65,\n",
       " [Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 0}, page_content='arXiv:2002.08909v1  [cs.CL]  10 Feb 2020\\nREALM: Retrieval-Augmented Language Model Pre-T raining\\nKelvin Guu * 1 Kenton Lee * 1 Zora T ung1 Panupong Pasupat 1 Ming-W ei Chang1\\nAbstract\\nLanguage model pre-training has been shown to\\ncapture a surprising amount of world knowledge,\\ncrucial for NLP tasks such as question answer-\\ning. However, this knowledge is stored implic-\\nitly in the parameters of a neural network, requir-\\ning ever-larger networks to cover more facts. T o\\ncapture knowledge in a more modular and inter-\\npretable way, we augment language model pre-\\ntraining with a latentknowledge retriever , which\\nallows the model to retrieve and attend over doc-\\numents from a large corpus such as Wikipedia,\\nused during pre-training, ﬁne-tuning and infer-\\nence. For the ﬁrst time, we show how to pre-\\ntrain such a knowledge retriever in an unsuper-\\nvised manner, using masked language model-\\ning as the learning signal and backpropagating\\nthrough a retrieval step that considers millions'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 0}, page_content='train such a knowledge retriever in an unsuper-\\nvised manner, using masked language model-\\ning as the learning signal and backpropagating\\nthrough a retrieval step that considers millions\\nof documents. W e demonstrate the effective-\\nness of Retrieval-Augmented Language Model\\npre-training (REALM) by ﬁne-tuning on the chal-\\nlenging task of Open-domain Question Answer-\\ning (Open-QA). W e compare against state-of-the-\\nart models for both explicit and implicit knowl-\\nedge storage on three popular Open-QA bench-\\nmarks, and ﬁnd that we outperform all previous\\nmethods by a signiﬁcant margin (4-16% absolute\\naccuracy), while also providing qualitative bene-\\nﬁts such as interpretability and modularity.\\n1. Introduction\\nRecent advances in language model pre-training have\\nshown that models such as BER T (\\nDevlin et al. , 2018),\\nRoBER T a ( Liu et al. , 2019) and T5 ( Raffel et al. , 2019)\\nstore a surprising amount of world knowledge, ac-\\nquired from the massive text corpora they are trained\\non ('),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 0}, page_content='Devlin et al. , 2018),\\nRoBER T a ( Liu et al. , 2019) and T5 ( Raffel et al. , 2019)\\nstore a surprising amount of world knowledge, ac-\\nquired from the massive text corpora they are trained\\non (\\nPetroni et al. , 2019). For example, BER T is able to\\n* Equal contribution 1 Google Research. Correspondence\\nto: Kelvin Guu <kguu@google.com>, Kenton Lee <ken-\\ntonl@google.com>, Zora Tung <gatoatigrado@google.com>,\\nPanupong Pasupat <ppasupat@google.com>, Ming-W ei Chang\\n<mingweichang@google.com>.\\nFigure 1. REALM augments language model pre-training with\\na neural knowledge retriever that retrieves knowledge from a\\ntextual knowledge corpus , Z (e.g., all of Wikipedia). Signal\\nfrom the language modeling objective backpropagates all th e way\\nthrough the retriever, which must consider millions of docu ments\\nin Z—a signiﬁcant computational challenge that we address.\\ncorrectly predict the missing word in the following sen-\\ntence: “The\\nis the currency of the United\\nKingdom” (answer: “ pound”).'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 0}, page_content='in Z—a signiﬁcant computational challenge that we address.\\ncorrectly predict the missing word in the following sen-\\ntence: “The\\nis the currency of the United\\nKingdom” (answer: “ pound”).\\nIn these language models, the learned world knowledge is\\nstoredimplicitly in the parameters of the underlying neural\\nnetwork. This makes it difﬁcult to determine what knowl-\\nedge is stored in the network and where. Furthermore, stor-\\nage space is limited by the size of the network—to cap-\\nture more world knowledge, one must train ever-larger net-\\nworks, which can be prohibitively slow or expensive.\\nT o capture knowledge in a more interpretable and modular\\nway, we propose a novel framework, Retrieval-Augmented\\nLanguage Model (REALM) pre-training, which augments\\nlanguage model pre-training algorithms with a learnedtex-\\ntual knowledge retriever . In contrast to models that store\\nknowledge in their parameters, this approach explicitly ex-\\nposes the role of world knowledge by asking the model to'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\ndecide what knowledge to retrieve and use during inference.\\nBefore making each prediction, the language model uses\\nthe retriever to retrieve documents\\n1 from a large corpus\\nsuch as Wikipedia, and then attends over those documents\\nto help inform its prediction. Learning this model end-to-\\nend requires backpropagating through a retrieval step that\\nconsiders an entire corpus of textual knowledge, as shown\\nin Figure\\n1.\\nThe key intuition of REALM is to train the retriever us-\\ning a performance-based signal from unsupervised text:\\na retrieval that improves the language model’s perplex-\\nity is helpful and should be rewarded, while an un-\\ninformative retrieval should be penalized. For exam-\\nple, in Figure\\n1, if the model needs to ﬁll the blank\\nin “ the at the top of the pyramid”, the re-\\ntriever should be rewarded for selecting a document con-\\ntaining “The pyramidion on top allows for less\\nmaterial higher up the pyramid”. W e achieve this'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='in “ the at the top of the pyramid”, the re-\\ntriever should be rewarded for selecting a document con-\\ntaining “The pyramidion on top allows for less\\nmaterial higher up the pyramid”. W e achieve this\\nbehavior by modeling our retrieve-then-predict approach\\nas a latent variable language model and optimizing the\\nmarginal likelihood.\\nIncorporating a large-scale neural retrieval module during\\npre-training constitutes a signiﬁcant computational chal -\\nlenge, since the retriever must consider millions of candi-\\ndate documents for each pre-training step, and we must\\nbackpropagate through its decisions. T o address this, we\\nstructure the retriever such that the computation performed\\nfor each document can be cached and asynchronously up-\\ndated, and selection of the best documents can be formu-\\nlated as Maximum Inner Product Search (MIPS).\\nNumerous prior works have demonstrated the bene-\\nﬁt of adding a discrete retrieval step to neural net-\\nworks ('),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='lated as Maximum Inner Product Search (MIPS).\\nNumerous prior works have demonstrated the bene-\\nﬁt of adding a discrete retrieval step to neural net-\\nworks (\\nMiller et al. , 2016; Chen et al. , 2017), but did not\\napply the framework to language model pre-training and\\nemployed non-learned retrievers to handle large-scale doc-\\nument collections. In the language modeling literature, th e\\nk-Nearest Neighbor Language Model (\\nKhandelwal et al. ,\\n2019) ( kNN-LM) retrieves similar LM examples to im-\\nprove memorization. However, kNN-LM was not ﬁne-\\ntuned for downstream tasks, perhaps because it is unclear\\nhow to adapt the retrieval mechanism: akNN can only use\\nexamples labeled for the target task—during ﬁne-tuning,\\nthis precludes LM examples, which contain the desired\\nworld knowledge. In contrast, REALM’s retriever is de-\\nsigned to transfer to other tasks, and the retrieval is just\\ntext, not a labeled example.\\nW e evaluate our approach by ﬁne-tuning the mod-'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='world knowledge. In contrast, REALM’s retriever is de-\\nsigned to transfer to other tasks, and the retrieval is just\\ntext, not a labeled example.\\nW e evaluate our approach by ﬁne-tuning the mod-\\nels pre-trained with REALM on the task of Open-\\ndomain Question Answering (Open-QA), one of the most\\nknowledge-intensive tasks in natural language process-\\ning. W e evaluate on three popular Open-QA bench-\\nmarks (NAT U R A LQU E S T I O N S -O P EN , WE B QU E S T I O N S , and\\nCU R AT E DTR E C) and compare to state-of-the-art Open-QA\\nmodels, including both extremely large models that store\\nknowledge implicitly (such as T5) as well as previous ap-\\nproaches that also use a knowledge retriever to access ex-\\nternal knowledge, but implement retrieval in a more heuris-\\ntic fashion (\\nLee et al. , 2019; Min et al. , 2019a; Asai et al. ,\\n2019). REALM achieves new state-of-the-art results on all\\nthree benchmarks, signiﬁcantly outperforming all previou s'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='tic fashion (\\nLee et al. , 2019; Min et al. , 2019a; Asai et al. ,\\n2019). REALM achieves new state-of-the-art results on all\\nthree benchmarks, signiﬁcantly outperforming all previou s\\nsystems by 4-16% absolute accuracy. W e also demonstrate\\nqualitative beneﬁts of REALM, including interpretability\\nand modularity.\\n2. Background\\nLanguage model pre-trainingThe goal of language\\nmodel pre-training is to learn useful representations of la n-\\nguage, usually from unlabeled text corpora. The resulting\\npre-trained model can then be further trained (ﬁne-tuned )\\nfor a downstream task of primary interest (in our case,\\nOpen-QA), often leading to better generalization than train-\\ning from scratch (\\nDai & Le , 2015; Radford et al. , 2019).\\nW e focus on the masked language model 2 (MLM) variant\\nof pre-training popularized by BER T ( Devlin et al. , 2018).\\nIn its basic form, an MLM is trained to predict the miss-\\ning tokens in an input text passage. Given an unlabeled'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='of pre-training popularized by BER T ( Devlin et al. , 2018).\\nIn its basic form, an MLM is trained to predict the miss-\\ning tokens in an input text passage. Given an unlabeled\\npre-training corpusX (e.g., Wikipedia text), a training ex-\\nample (x, y ) can be generated by randomly masking to-\\nkens in a sampled piece of text (e.g., x = “ The [MASK]\\nis the currency [MASK] the UK”; y = (“ pound”,\\n“ of”)). The model uses its representation of the masked\\ninput x to predict the token that should go in each mask.\\nA good MLM must learn to encode syntactic and semantic\\ninformation (e.g., to predict “ of”) as well as some world\\nknowledge (e.g., to predict “ pound”).\\nOpen-domain question answering (Open-QA) T o mea-\\nsure a model’s ability to incorporate world knowledge, we\\nneed a downstream task where world knowledge is criti-\\ncal. Perhaps one of the most knowledge-intensive tasks in\\nnatural language processing is open-domain question an-\\nswering (Open-QA): given a questionx such as “ What is')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits), all_splits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['90a11686-d10b-4382-a8b7-d6786354e55c',\n",
       " '00ba0c9b-6b25-4840-b7e0-3a2fddb64f74',\n",
       " '7f39a146-ecca-41cb-bc5c-7322cea3e472',\n",
       " '5ebcfb0c-bcd6-4c69-b303-795fd6de02cd',\n",
       " 'f8cc9cf0-eb6e-4aa4-a530-eb7357eb4243',\n",
       " 'a185cfe4-6954-498d-a34b-f88ab3a2aa91',\n",
       " 'd329f9cc-23fd-44de-b22f-7a0c47870dad',\n",
       " '6b0d8e26-f04d-4088-9906-9e82861a4fc4',\n",
       " '29285878-9554-4a29-9981-91ec5f8f7877',\n",
       " 'a9c8bc7c-8088-4bf0-a488-41e84be1e28b',\n",
       " '7f97b4b4-2cd1-476b-8c07-b5966cab66fc',\n",
       " '059dcdf5-502c-44de-b7ee-8ffed3c84aa7',\n",
       " '837947c2-ab15-46f8-ae24-c43911ad8b64',\n",
       " '9fdd1efc-c19f-45bc-b769-be0ac797f1a3',\n",
       " '825c7401-ac9a-412a-a3c5-77b2f0323747',\n",
       " '7ba0538c-37be-477e-97a1-17925fcb796a',\n",
       " '3ceb3d68-4565-4da8-aaa7-0b909f68a71b',\n",
       " 'fe473b29-9bb2-40cb-93fa-311d4f7c14c2',\n",
       " '83eb77ad-bba5-444e-8977-a926baae0f3c',\n",
       " '85fab6ac-4553-46a0-beda-2a6caedf7c9b',\n",
       " '0d58e95f-689f-4d4c-82ca-a711c812a9a2',\n",
       " '16983694-b638-4048-8f50-febd32c507cc',\n",
       " '4b44bdd5-baac-49e7-a026-b9c1e06a7375',\n",
       " 'f0add0f8-f3ef-40b8-84c5-8ae119b72327',\n",
       " '8b0e04ab-c271-480c-890a-e1e53f3c23b0',\n",
       " 'c4ff1e9c-b3d7-43b9-9360-0c58b56cda3f',\n",
       " 'dfdff2f9-89e6-4e6c-ae74-84cb8e796919',\n",
       " 'c1ddd2b2-2087-479a-8d81-20cec74c001b',\n",
       " 'aab63980-0109-443e-bc0a-998032f86bdb',\n",
       " 'b9848938-a75b-484c-8862-7dbb4782b11e',\n",
       " 'bffb6661-54e4-4f58-a6b2-986a4c286903',\n",
       " 'd3ec4290-88e0-4801-a8ac-03a821297370',\n",
       " '9062ad4b-754e-4a50-80b9-59ea2472d5a5',\n",
       " '89cc9220-2c55-4ae1-a931-bdce3d67be67',\n",
       " '93441ddf-868a-403f-9b8c-3d23e38d970d',\n",
       " 'ce920aca-aaae-4bae-a61d-166a18409f90',\n",
       " '4725ab85-7641-4330-a0ae-0f1adf3ee3e5',\n",
       " '48628d06-7db9-4cb9-a543-9afd2218e591',\n",
       " 'ded2f83c-a570-45df-b3e9-474c5a17216b',\n",
       " '39bf7c5f-4aff-46ad-89d5-0b114803e823',\n",
       " '3b3ca114-d63b-4d36-a596-a0d36cd01051',\n",
       " '9a05f169-619f-4a3d-9ca8-e088e574f19c',\n",
       " '33142fee-1306-4c20-b711-6215bfae5ddc',\n",
       " '07feaa0b-5645-44d8-bc19-c86feb9bd0ee',\n",
       " '22d12741-6891-47af-a2a1-7f002bc2cef9',\n",
       " '4d813376-88c6-4cfa-a48a-0f0d245a3675',\n",
       " 'b1d11e7d-6ee9-4749-b9a6-a1139d14caad',\n",
       " 'a9bdc3d7-4146-40be-9028-c01a205957d2',\n",
       " '1e699b94-b9b1-4f30-a166-49a006b58080',\n",
       " '7210bedd-66f1-4ad2-8a24-d3ee32b40e78',\n",
       " 'd5d17f7f-d282-46f3-8f22-c0c57eab3951',\n",
       " '64ab9ff8-e91b-45d5-8e5d-6da44d37a1e5',\n",
       " 'f1220549-03c3-4906-9722-df2b42e60c91',\n",
       " 'b0ee6380-da90-431b-ac00-c0b28697cdf6',\n",
       " 'b1a79fff-5ab0-478f-b23a-6c44c331f5a7',\n",
       " '7ade0ae6-82c5-4b6b-9028-dcd079e12cf5',\n",
       " 'ac872308-819c-43e2-9bc9-1c105448c3a6',\n",
       " '395d54b2-783e-4001-986c-82476c8545de',\n",
       " '078ab4d4-6989-4f5c-b824-2d1fb0247233',\n",
       " '7e5b3a76-50d9-4270-841e-b7c979aafa00',\n",
       " '1cee6a04-1f8c-4b95-8767-34e6b5c46e16',\n",
       " '7e9b383a-81de-4b3b-86dc-53295c7ea9e1',\n",
       " '2f3dc59f-bd65-4189-b5b0-00e92d4dd25d',\n",
       " '16d4b340-7e17-481b-9b3d-d8006162b333',\n",
       " 'c201e79d-e1c3-4103-a857-6a6f90102f37']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, api_key\n",
    "os.environ['OPENAI_API_KEY'] = api_key.OPENAI_API_KEY\n",
    "# Init llm, embeddings, vector_store\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "vector_store.add_documents(all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        'system',\n",
    "        'You are an expert in answering questions based on the provided context,'\n",
    "         'if you do not know the answer to a question, you can say you dont know.'\n",
    "         'Use maximum 3 sentences to answer, keep your answer concise.'\n",
    "    ),\n",
    "    (\n",
    "        'user',\n",
    "        \"Question: {question} \\nContext: {context}\"\n",
    "    )\n",
    "]) \n",
    "def extract_context(question) -> List[Document]:\n",
    "    context = vector_store.similarity_search(\n",
    "        query=question,\n",
    "    )\n",
    "    return context\n",
    "\n",
    "def generate(question, context) -> str:\n",
    "\n",
    "    message_retrieve_context= prompt.invoke(\n",
    "        {\"question\": question, 'context': context})\n",
    "    \n",
    "    response = llm.invoke(message_retrieve_context)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What problem in the field that this paper aiming to solve? \n",
      "Context: [Document(id='29285878-9554-4a29-9981-91ec5f8f7877', metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='tic fashion (\\nLee et al. , 2019; Min et al. , 2019a; Asai et al. ,\\n2019). REALM achieves new state-of-the-art results on all\\nthree benchmarks, signiﬁcantly outperforming all previou s\\nsystems by 4-16% absolute accuracy. W e also demonstrate\\nqualitative beneﬁts of REALM, including interpretability\\nand modularity.\\n2. Background\\nLanguage model pre-trainingThe goal of language\\nmodel pre-training is to learn useful representations of la n-\\nguage, usually from unlabeled text corpora. The resulting\\npre-trained model can then be further trained (ﬁne-tuned )\\nfor a downstream task of primary interest (in our case,\\nOpen-QA), often leading to better generalization than train-\\ning from scratch (\\nDai & Le , 2015; Radford et al. , 2019).\\nW e focus on the masked language model 2 (MLM) variant\\nof pre-training popularized by BER T ( Devlin et al. , 2018).\\nIn its basic form, an MLM is trained to predict the miss-\\ning tokens in an input text passage. Given an unlabeled'), Document(id='a185cfe4-6954-498d-a34b-f88ab3a2aa91', metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='in “ the at the top of the pyramid”, the re-\\ntriever should be rewarded for selecting a document con-\\ntaining “The pyramidion on top allows for less\\nmaterial higher up the pyramid”. W e achieve this\\nbehavior by modeling our retrieve-then-predict approach\\nas a latent variable language model and optimizing the\\nmarginal likelihood.\\nIncorporating a large-scale neural retrieval module during\\npre-training constitutes a signiﬁcant computational chal -\\nlenge, since the retriever must consider millions of candi-\\ndate documents for each pre-training step, and we must\\nbackpropagate through its decisions. T o address this, we\\nstructure the retriever such that the computation performed\\nfor each document can be cached and asynchronously up-\\ndated, and selection of the best documents can be formu-\\nlated as Maximum Inner Product Search (MIPS).\\nNumerous prior works have demonstrated the bene-\\nﬁt of adding a discrete retrieval step to neural net-\\nworks ('), Document(id='4d813376-88c6-4cfa-a48a-0f0d245a3675', metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 7}, page_content='amples of this progression include models that condi-\\ntion on surrounding words (\\nMikolov et al. , 2013a;b), sen-\\ntences ( Kiros et al. , 2015; Peters et al. , 2018), and para-\\ngraphs ( Radford et al. , 2018; Devlin et al. , 2018). W e can\\nview REALM as a generalization of the above work to the\\nnext level of scope: the entire text corpus.\\nRetrieve-and-edit with learned retrieval In order to\\nbetter explain the variance in the input text and en-\\nable controllable generation,\\nGuu et al. (2018) proposed\\na language model with the retrieve-and-edit frame-\\nwork ( Hashimoto et al. , 2018) that conditions on text with\\nhigh lexical overlap. REALM has a similar approach, ex-\\ncept that the model learns for itself which texts are most\\nuseful for reducing perplexity. By jointly learning the re-\\ntriever, REALM has the capacity to depend on information\\nbeyond lexical overlap.\\nScalable grounded neural memoryThe document in-\\ndex can be viewed as a memory where the keys are'), Document(id='f8cc9cf0-eb6e-4aa4-a530-eb7357eb4243', metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 1}, page_content='REALM: Retrieval-Augmented Language Model Pre-T raining\\ndecide what knowledge to retrieve and use during inference.\\nBefore making each prediction, the language model uses\\nthe retriever to retrieve documents\\n1 from a large corpus\\nsuch as Wikipedia, and then attends over those documents\\nto help inform its prediction. Learning this model end-to-\\nend requires backpropagating through a retrieval step that\\nconsiders an entire corpus of textual knowledge, as shown\\nin Figure\\n1.\\nThe key intuition of REALM is to train the retriever us-\\ning a performance-based signal from unsupervised text:\\na retrieval that improves the language model’s perplex-\\nity is helpful and should be rewarded, while an un-\\ninformative retrieval should be penalized. For exam-\\nple, in Figure\\n1, if the model needs to ﬁll the blank\\nin “ the at the top of the pyramid”, the re-\\ntriever should be rewarded for selecting a document con-\\ntaining “The pyramidion on top allows for less\\nmaterial higher up the pyramid”. W e achieve this')] \n",
      "Answer: The paper aims to solve the problem of improving language model performance by integrating a retrieval mechanism that allows the model to access and utilize external knowledge from large corpora, like Wikipedia, during inference. This approach addresses the challenges of incorporating large-scale neural retrieval, optimizing the selection of useful documents, and enhancing the model's ability to generate accurate predictions. By training the retrieval process alongside the language model, the authors seek to improve overall accuracy and interpretability in tasks such as Open-QA.\n"
     ]
    }
   ],
   "source": [
    "question = \"What problem in the field that this paper aiming to solve?\"\n",
    "\n",
    "context = extract_context(question=question)\n",
    "\n",
    "response = generate(question=question, context=context)\n",
    "\n",
    "print(f\"Question: {question} \\nContext: {context} \\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text splitter is good enough for scientific paper, since it is written English, other more complex languages include coding, markdown, HTML, .etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing_extensions import TypedDict\n",
    "prompt = ChatPromptTemplate([\n",
    "    ('system',\n",
    "     \"You are an expert as answering question based on the provided context\"\n",
    "     \"Your answer should be no more than 3 sentences, try to keep it concise\"\n",
    "     \"If you do not know any part of the question, just say you do not know.\"),\n",
    "     (\n",
    "         'user',\n",
    "         \"Question: {question}\\nContext: {context}\\nAnswer: \"\n",
    "     )\n",
    "])\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve_context(state: State):\n",
    "    query = state['question']\n",
    "\n",
    "    context = vector_store.similarity_search(query=query)\n",
    "    return {'context': context}\n",
    "\n",
    "def generate(state: State):\n",
    "    question = state['question']\n",
    "    context =  state['context']\n",
    "    context_str = '\\n\\n'.join([doc.page_content for doc in context])\n",
    "    input_messages = prompt.invoke({\n",
    "        \"question\": question,\n",
    "        \"context\": context_str\n",
    "    })\n",
    "    response = llm.invoke(input_messages)\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve_context, generate]).add_edge(START, 'retrieve_context')\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what are score that achived SOTA in the paper?',\n",
       " 'context': [Document(id='3b3ca114-d63b-4d36-a596-a0d36cd01051', metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 6}, page_content='after ablating critical components of REALM. In addition\\nto the end-to-end results, we also report how often the gold\\nanswer appears in the top-5 retrievals before applying any\\nﬁne-tuning. The latter metric more signiﬁcantly isolates the\\ncontribution of improving the retriever during pre-traini ng.'),\n",
       "  Document(id='4725ab85-7641-4330-a0ae-0f1adf3ee3e5', metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 6}, page_content='DrQA ( Chen et al. , 2017) Sparse Retr. +DocReader N/A - 20.7 25.7 34m\\nHardEM ( Min et al. , 2019a) Sparse Retr. +Transformer BERT 28.1 - - 110m\\nGraphRetriever ( Min et al. , 2019b) GraphRetriever +Transformer BERT 31.8 31.6 - 110m\\nPathRetriever ( Asai et al. , 2019) PathRetriever +Transformer MLM 32.6 - - 110m\\nORQA ( Lee et al. , 2019) Dense Retr. +Transformer ICT +BERT 33.3 36.4 30.1 330m\\nOurs ( X = Wikipedia, Z = Wikipedia) Dense Retr. +Transformer REALM 39.2 40.2 46.8 330m\\nOurs ( X = CC-News, Z = Wikipedia) Dense Retr. +Transformer REALM 40.4 40.7 42.9 330m\\nT able 2. Ablation experiments on NQ’s development set.\\nAblation Exact\\nMatch\\nZero-shot\\nRetrieval\\nRecall@5\\nREALM 38.2 38.5\\nREALM retriever +Baseline encoder 37.4 38.5\\nBaseline retriever +REALM encoder 35.3 13.9\\nBaseline (ORQA) 31.3 13.9\\nREALM with random uniform masks 32.3 24.2\\nREALM with random span masks 35.3 26.1\\n30× stale MIPS 28.7 15.1\\nentire model can be run on a single machine with a 12GB\\nGPU.'),\n",
       "  Document(id='48628d06-7db9-4cb9-a543-9afd2218e591', metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 6}, page_content='Baseline (ORQA) 31.3 13.9\\nREALM with random uniform masks 32.3 24.2\\nREALM with random span masks 35.3 26.1\\n30× stale MIPS 28.7 15.1\\nentire model can be run on a single machine with a 12GB\\nGPU.\\nPre-trainingW e pre-train for 200k steps on 64 Google\\nCloud TPUs, with a batch size of 512 and a learning rate\\nof 3e-5, using BER T’s default optimizer. The document\\nembedding step for the MIPS index is parallelized over 16\\nTPUs. For each example, we retrieve and marginalize over\\n8 candidate documents, including the null document ∅ .\\nW e experiment with two choices of the pre-training corpus\\nX : (1) Wikipedia, which is identical to the knowledge cor-\\npus Z, and (2) CC-News, our reproduction of the corpus of\\nEnglish news proposed by\\nLiu et al. (2019).\\n4.4. Main results\\nT able 1 shows the accuracy of different approaches on the\\nthree Open-QA datasets. REALM outperform all previous\\napproaches by a signiﬁcant margin. T able\\n1 also shows the\\nnumber of parameters for each model.'),\n",
       "  Document(id='ded2f83c-a570-45df-b3e9-474c5a17216b', metadata={'source': 'https://arxiv.org/pdf/2002.08909', 'page': 6}, page_content='three Open-QA datasets. REALM outperform all previous\\napproaches by a signiﬁcant margin. T able\\n1 also shows the\\nnumber of parameters for each model.\\nAs reported in the concurrent work of Roberts et al. (2020),\\nthe generative Open-QA systems based on T5 are surpris-\\ningly powerful, with the largest T5-11B model outperform-\\ning the previous best Open-QA system. Increasing the size\\nof T5 yields consistent improvement, but comes at signif-\\nicant computational cost (from Base to 11B, the model is\\n50 times larger, and gains roughly 5 points in accuracy). In\\ncontrast, REALM outperforms the largest T5-11B model\\nwhile being 30 times smaller. It is also important to note\\nthat T5 accesses additional reading comprehension data\\nfrom SQuAD during its pre-training (100,000+ examples).\\nAccess to such data could also beneﬁt REALM, but was not\\nused in our experiments.\\nAmong all systems, the most direct comparison with\\nREALM is ORQA (\\nLee et al. , 2019), where the ﬁne-tuning')],\n",
       " 'answer': AIMessage(content='The paper reports that REALM achieved scores of 39.2 for Exact Match and 40.2 for Zero-shot Retrieval when using Wikipedia as the pre-training corpus, and scores of 40.4 for Exact Match and 40.7 for Zero-shot Retrieval when using CC-News. These scores represent state-of-the-art performance compared to previous systems.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 1049, 'total_tokens': 1121, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-aa8afb2b-64f1-4041-a8d9-ca03d75c0495-0', usage_metadata={'input_tokens': 1049, 'output_tokens': 72, 'total_tokens': 1121, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({\n",
    "    'question': \"what are score that achived SOTA in the paper?\"\n",
    "}, config={'configurable': {'thread_id': 'thread0123'}})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step:\n",
    "- Learn RAG #2 tutorial\n",
    "- Build a simple streamlit UI, to test each model / pipeline that I learn later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG #2 tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, api_key\n",
    "os.environ['OPENAI_API_KEY'] = api_key.OPENAI_API_KEY\n",
    "# Init llm, embeddings, vector_store\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_API_KEY'] = api_key.LANGSMITH_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5dbf71fe-8b5d-4b4d-8fd9-7eed50927eed',\n",
       " '5b214bcb-0ef3-4ea6-ab4d-acb216fea43d',\n",
       " 'c763dd61-c62b-4c44-a3c7-d1aa2b380bbf',\n",
       " 'fcbfedb6-3ee1-432b-9fb5-abccc6df7a99',\n",
       " '37359215-7210-4bf0-abfa-b6262ab7b5f4',\n",
       " 'd49d5cf3-0d6a-47b3-b2f3-b794115222ca',\n",
       " 'd0fda22d-5956-4190-8f84-ecb90d696654',\n",
       " '1b5e914b-427e-4867-881f-29411efe8a8f',\n",
       " '604eb190-ec79-4521-86f8-57c5bc07ff73',\n",
       " '4d175b7c-1011-4b6b-bda1-0b7c9d9738b5',\n",
       " 'ffa89a4e-93dc-4336-a2c0-1269d3f4ae7c',\n",
       " 'd60fd57b-d3df-40b1-861c-b41c3f626095',\n",
       " '027579f1-7b75-4ebc-89d0-8960ea7438d3',\n",
       " '395ab9f6-1d7f-4e64-954b-bc88f5fe35c2',\n",
       " '72d3564f-3ca8-4e2c-81bd-04f5e64e4e4c',\n",
       " '3ae76397-dffd-4970-ae70-a311051c1e37',\n",
       " 'ef240638-8bf5-49fb-aee6-cbad6c95d4ef',\n",
       " '26967043-5506-43a9-8612-780e9131b0b9',\n",
       " 'e62f2c77-b144-47f9-ad70-067bfdb93959',\n",
       " 'e6b0a042-6940-485e-a559-7368d1c56159',\n",
       " '7f075be1-6a22-4d0c-9b27-93940500c5cc',\n",
       " '894f4f44-2887-415e-bd2a-f938715f1cde',\n",
       " 'ddb6eccd-753f-4626-8453-2dfce7807d04',\n",
       " 'fb2cb865-df7a-4450-91b8-35bb5858f5bb',\n",
       " '78111090-2d43-4679-b9f5-3b21aa551346',\n",
       " 'd1083832-7d7c-45a6-9aa4-6bba00987b4b',\n",
       " '4c950d37-f104-4227-95e0-efd47b287720',\n",
       " 'c9e03739-2117-421e-bda3-7ecce68f0d01',\n",
       " '0aed00cb-b486-4295-b587-cfc1e1b640fe',\n",
       " 'fe2edc6f-f64e-4ece-b970-afc0576d79ae',\n",
       " '719fe158-a7a1-4765-a3c4-e8099fe298dd',\n",
       " 'e910e377-430c-4f49-832b-1704066aa5ee',\n",
       " '7a96cf9b-5391-449c-88e5-a2c3492092bb',\n",
       " '625c7764-59cb-4e34-aea8-9020b7861496',\n",
       " 'fa984901-da29-4fd5-9f2f-18cac8033e59',\n",
       " 'e7bde2a7-0222-4a1e-a33a-dc2e69036faa',\n",
       " 'b46b9697-9149-4368-a587-a9999294bf1e',\n",
       " '208cfae4-6eb0-4ece-830e-69be6819779b',\n",
       " 'aeb38704-9adc-45ea-843f-6458cda9cd23',\n",
       " '6f6c3cce-09fe-4e92-9850-122daa40c570',\n",
       " '82afbf27-3ebd-45d1-b1e4-75ac8dd4e066',\n",
       " '38c868a3-7091-463b-940b-9346a5073996',\n",
       " '080e71c0-908a-4bc5-9999-be2440fe3444',\n",
       " '5c66589a-07d8-4725-b595-8a3df99f1708',\n",
       " 'dc4be88c-6ae3-489a-82e7-afd4c3cc1ff8',\n",
       " 'd44b03b4-8d56-41db-bcbc-09eaa6c4f11b',\n",
       " 'd476a151-1963-4f05-b693-15cec53cdbb9',\n",
       " '68c64228-a695-4d5e-b684-1df0a4dd0663',\n",
       " 'cbf506b9-8bb9-47af-8c59-5d191c36e7e9',\n",
       " 'e728ef70-2ab1-4a2a-ae3e-27e0b1057d45',\n",
       " '7ff8d0c0-061e-4aad-a245-768499f7a9ca',\n",
       " '9264ddbc-52f9-4886-b3e5-dc7c30a7625d',\n",
       " 'd3362a6c-fc25-41e5-92f1-4aac0fa60db1',\n",
       " '9aa6cba1-04e9-4fb0-9b4d-aa70082f7672',\n",
       " 'b8d02dea-ebbd-4cee-b21d-4ff7840c17b5',\n",
       " '9810e62a-efd9-460c-95ee-9f0b102155a6',\n",
       " '85f1d2e9-ee51-464f-bcca-2b8d4eead539',\n",
       " '04d2ed04-bba2-4d44-a03f-18be7f07c050',\n",
       " '65fab262-d358-4a62-8d92-859653c0adde',\n",
       " '9a0cc0a9-acdb-43f1-8655-1dea7063e7e6',\n",
       " '664ef60f-f3b2-48d4-80ab-ef80da8e9887',\n",
       " 'eaa9adc5-e9de-4ed7-a4cd-0963e7958319',\n",
       " 'b3f4be7f-85d1-4d1a-ae27-36cc401d9d7e',\n",
       " '43328224-f5b4-41e8-bfc9-8f114f474cee',\n",
       " 'a2e1beb1-50d8-4921-89ee-d53ac43e6801',\n",
       " '664dc9ef-97d0-42cf-bdca-7d99ef6e3a21']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to query\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join( [f\"Source: {doc.metadata}\\nContent: {doc.page_content}\"\n",
    "                               for doc in retrieved_docs])\n",
    "    return serialized, retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond directly\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state['messages'])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "\n",
    "    print(f\"We are in `generate` step\")\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state['messages']): # why need reversed?\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1] # reverse the list, why?\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "\n",
    "    system_message_content = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversational_messages = [\n",
    "        message \n",
    "        for message in state['messages']\n",
    "            if message.type in ('human', 'system')\n",
    "            or (message.type == 'ai' and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversational_messages\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return {'messages': [response]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\") # ? is this equivalent to? .add_edge(START, \"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"}\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "from IPython.display import display, Image\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}, id='bc08522a-a56a-4363-84c7-16a29c3caf88'),\n",
       "  AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 43, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-13ec030f-7515-477e-8542-dda4c6da7ec9-0', usage_metadata={'input_tokens': 43, 'output_tokens': 11, 'total_tokens': 54, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_message = \"Hello\"\n",
    "config = {\"configurable\": {\"thread_id\": \"user1234\"}}\n",
    "output = graph.invoke({\"messages\": input_message}, config=config)   \n",
    "\n",
    "output['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Task Decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_9u3eXNPGxmT77YSIDSdSJJup)\n",
      " Call ID: call_9u3eXNPGxmT77YSIDSdSJJup\n",
      "  Args:\n",
      "    query: Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "We are in `generate` step\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. This is often enhanced by techniques like Chain of Thought (CoT), which prompts models to think step by step, making it easier to tackle complex problems. It can be done through simple prompts, task-specific instructions, or human inputs.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you look up some common ways of doing it?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_xIrgu9qMOqUwupWWcdGRSvb1)\n",
      " Call ID: call_xIrgu9qMOqUwupWWcdGRSvb1\n",
      "  Args:\n",
      "    query: common ways of task decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "We are in `generate` step\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Common ways of performing Task Decomposition include using simple prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\", employing task-specific instructions such as \"Write a story outline\" for specific tasks, and incorporating human inputs to guide the decomposition process. These methods help to clarify and structure the approach to complex tasks.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Can you look up some common ways of doing it?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporate Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n",
    "\n",
    "display(Image(agent_executor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.' additional_kwargs={} response_metadata={} id='9f01cc31-286b-4a74-b16d-9fc97951aeaf'\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_GXUiHRlc2Wg8fFSdrCUdykzP', 'function': {'arguments': '{\"query\": \"standard method for Task Decomposition\"}', 'name': 'retrieve'}, 'type': 'function'}, {'id': 'call_Vuft3wxYGMfU4D1H78EsXptn', 'function': {'arguments': '{\"query\": \"common extensions of Task Decomposition methods\"}', 'name': 'retrieve'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 3756, 'total_tokens': 3810, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3584}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-ecce3051-74d0-427f-bdf2-bb118eb3cab6-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'standard method for Task Decomposition'}, 'id': 'call_GXUiHRlc2Wg8fFSdrCUdykzP', 'type': 'tool_call'}, {'name': 'retrieve', 'args': {'query': 'common extensions of Task Decomposition methods'}, 'id': 'call_Vuft3wxYGMfU4D1H78EsXptn', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3756, 'output_tokens': 54, 'total_tokens': 3810, 'input_token_details': {'audio': 0, 'cache_read': 3584}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.' name='retrieve' id='521a04ab-c9dd-4b87-a2e7-60314b8dbdda' tool_call_id='call_Vuft3wxYGMfU4D1H78EsXptn' artifact=[Document(id='fcbfedb6-3ee1-432b-9fb5-abccc6df7a99', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='c763dd61-c62b-4c44-a3c7-d1aa2b380bbf', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.')]\n",
      "content='The standard method for Task Decomposition is the Chain of Thought (CoT) technique. This method involves prompting the model to \"think step by step,\" which helps break down complex tasks into smaller, manageable steps.\\n\\nCommon extensions of this method include the Tree of Thoughts approach. This extension allows for exploring multiple reasoning possibilities at each step, decomposing the problem into various thought steps and generating multiple thoughts for each step, thereby creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state being evaluated by a classifier or majority vote.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 4516, 'total_tokens': 4638, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3712}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-ec356a7c-6fa0-424d-896c-dfba62f84789-0' usage_metadata={'input_tokens': 4516, 'output_tokens': 122, 'total_tokens': 4638, 'input_token_details': {'audio': 0, 'cache_read': 3712}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "input_message = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "config = {\"configurable\": {\"thread_id\": \"user123\"}}\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    # event[\"messages\"][-1].pretty_print()\n",
    "    print(event['messages'][-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
